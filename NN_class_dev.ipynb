{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_class_dev.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonSadowski/sc_Hierarchical_Classifier/blob/grid_search/NN_class_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sc_Hierarchical_Classifier"
      ],
      "metadata": {
        "id": "HiJpLlOX_d91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Needed Imports"
      ],
      "metadata": {
        "id": "Cw5TOzOv_qKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "import tensorflow.keras as keras \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import random\n",
        "from operator import add, sub"
      ],
      "metadata": {
        "id": "XAInitxPwiAO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class Neural_Network \n",
        "Used Model for Local Classifiers of Hierarchical Classifier"
      ],
      "metadata": {
        "id": "FeZG7IAvwWzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scvi"
      ],
      "metadata": {
        "id": "Old87Bn8NVTB",
        "outputId": "b4ebd602-0620-4d2b-9673-72028c35c592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_df = grid_search()"
      ],
      "metadata": {
        "id": "V1InX96fLsN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "vg-nRz7yx2Pg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6orUuY5L3Q4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_df"
      ],
      "metadata": {
        "id": "iY-P-pi0x5M2",
        "outputId": "ea3c0c84-fa08-45cc-928c-9bfe56c0fa97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    index  test_accuracy  option_scVI  n_neurons  n_layers  dropout  \\\n",
              "0       0       0.950573           20        128         2      0.2   \n",
              "1       1       0.940325           50         64         5      0.1   \n",
              "2       2       0.927667           50         64        10      0.2   \n",
              "3       3       0.926462           50        128         5      0.4   \n",
              "4       4       0.927667           50        256        10      0.1   \n",
              "..    ...            ...          ...        ...       ...      ...   \n",
              "95     95       0.934901           10         64         2      0.2   \n",
              "96     96       0.960217           10        256         5      0.4   \n",
              "97     97       0.913803           10         64         2      0.4   \n",
              "98     98       0.944545           20        256        10      0.1   \n",
              "99     99       0.939120           50        256         5      0.1   \n",
              "\n",
              "    learning_rate  momentum  batch_size  batch_norm  l2_reg  leakiness_ReLU  \n",
              "0           0.005       0.9          40       False   False             0.1  \n",
              "1           0.001       0.9          60        True   False             0.1  \n",
              "2           0.005       0.5          40       False   False             0.1  \n",
              "3           0.005       0.5          60        True    True             0.2  \n",
              "4           0.005       0.9          20       False    True             0.1  \n",
              "..            ...       ...         ...         ...     ...             ...  \n",
              "95          0.001       0.5          40       False   False             0.2  \n",
              "96          0.001       0.9          40       False   False             0.2  \n",
              "97          0.001       0.5          60       False    True             0.1  \n",
              "98          0.005       0.5          20       False   False             0.1  \n",
              "99          0.005       0.5          60        True    True             0.2  \n",
              "\n",
              "[100 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cbdda7bf-2191-463c-8594-94b967a11061\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>option_scVI</th>\n",
              "      <th>n_neurons</th>\n",
              "      <th>n_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>momentum</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>batch_norm</th>\n",
              "      <th>l2_reg</th>\n",
              "      <th>leakiness_ReLU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.950573</td>\n",
              "      <td>20</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>40</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.940325</td>\n",
              "      <td>50</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>60</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.927667</td>\n",
              "      <td>50</td>\n",
              "      <td>64</td>\n",
              "      <td>10</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>40</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.926462</td>\n",
              "      <td>50</td>\n",
              "      <td>128</td>\n",
              "      <td>5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>60</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.927667</td>\n",
              "      <td>50</td>\n",
              "      <td>256</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>0.934901</td>\n",
              "      <td>10</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.5</td>\n",
              "      <td>40</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>0.960217</td>\n",
              "      <td>10</td>\n",
              "      <td>256</td>\n",
              "      <td>5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>40</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>0.913803</td>\n",
              "      <td>10</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.5</td>\n",
              "      <td>60</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>0.944545</td>\n",
              "      <td>20</td>\n",
              "      <td>256</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>20</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>0.939120</td>\n",
              "      <td>50</td>\n",
              "      <td>256</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>60</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbdda7bf-2191-463c-8594-94b967a11061')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cbdda7bf-2191-463c-8594-94b967a11061 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cbdda7bf-2191-463c-8594-94b967a11061');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(\n",
        "  adata_source = 'drive/MyDrive/TNK_Zhang_BRCA_labeled_Test.h5ad',\n",
        "  options_scVI = [10, 20, 50],\n",
        "  options_n_neurons = [64, 128, 256],\n",
        "  options_n_layers = [2, 5, 10],\n",
        "  options_dropout = [0.1, 0.2, 0.4],\n",
        "  options_learning_rate = [0.001, 0.005],\n",
        "  options_momentum = [0.5, 0.9],\n",
        "  options_batch_size = [20, 40, 60],\n",
        "  options_batch_norm = [True, False],\n",
        "  options_l2_reg = [True, False],\n",
        "  options_leakiness_ReLU = [0.1, 0.2],\n",
        "  max_tries = 20,\n",
        "  random_tries = 100,\n",
        "):\n",
        "  \"\"\"\n",
        "  Run on network initialization, i.e. upon creation of a hierarchical classifier\n",
        "  and upon addition of a new node.\n",
        "  Rationale for doing this on a per-node basis: factors such as network depth\n",
        "  and number of neurons per hidden layer may be different with varying classification\n",
        "  complexity.\n",
        "  \"\"\"\n",
        "  adata = sc.read(adata_source)\n",
        "  adata = adata[adata.obs['Level_4'] != ''].copy()\n",
        "  for option_scVI in options_scVI:\n",
        "    if f'X_scVI_{option_scVI}' in adata.obsm:\n",
        "      continue\n",
        "    \n",
        "    scvi.model.SCVI.setup_anndata(adata, batch_key=\"batch\")\n",
        "    vae = scvi.model.SCVI(adata, n_latent=option_scVI)\n",
        "    vae.train(max_epochs=100)\n",
        "    adata.obsm[f'X_scVI_{option_scVI}'] = vae.get_latent_representation()\n",
        "  \n",
        "  adata.write(adata_source)\n",
        "  performance_df = pd.DataFrame(columns=['test_accuracy', 'option_scVI', 'n_neurons', 'n_layers', 'dropout', 'learning_rate', 'momentum', 'batch_size', 'batch_norm', 'l2_reg', 'leakiness_ReLU'])\n",
        "  for _ in range(random_tries):\n",
        "    current_model = keras.models.Sequential()\n",
        "    option_scVI = random.sample(options_scVI, 1) [0]\n",
        "    n_neurons = random.sample(options_n_neurons, 1)[0]\n",
        "    n_layers = random.sample(options_n_layers, 1)[0]\n",
        "    dropout = random.sample(options_dropout, 1)[0]\n",
        "    learning_rate = random.sample(options_learning_rate, 1)[0]\n",
        "    momentum = random.sample(options_momentum, 1)[0]\n",
        "    batch_size = random.sample(options_batch_size, 1)[0]\n",
        "    batch_norm = random.sample(options_batch_norm, 1)[0]\n",
        "    l2_reg = random.sample(options_l2_reg, 1)[0]\n",
        "    leakiness_ReLU = random.sample(options_leakiness_ReLU, 1)[0]\n",
        "    current_NN_helper = Neural_Network(\n",
        "      adata.obsm[f'X_scVI_{option_scVI}'],\n",
        "      adata.obs['Level_4'],\n",
        "      5\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    neurons_per_layer = [len(current_NN_helper.x_input_data[0])]\n",
        "    neurons_per_layer += n_layers * [n_neurons]\n",
        "    neurons_per_layer += [current_NN_helper.len_of_output]\n",
        "    for i, n_neurons_current in enumerate(neurons_per_layer):\n",
        "      if i == len(neurons_per_layer) - 2:\n",
        "        activation = 'softmax'\n",
        "      \n",
        "      elif i == len(neurons_per_layer) - 1:\n",
        "        break\n",
        "\n",
        "      else:\n",
        "        activation = lambda x: keras.activations.relu(x, alpha=leakiness_ReLU)\n",
        "\n",
        "      current_model.add(\n",
        "          keras.layers.Dense(\n",
        "              input_shape=(n_neurons_current, ),\n",
        "              units=neurons_per_layer[i + 1],\n",
        "              kernel_initializer='glorot_uniform',\n",
        "              bias_initializer='zeros',\n",
        "              activation=activation,\n",
        "              activity_regularizer=keras.regularizers.l2(l2=0.01) if l2_reg == True else None\n",
        "          )\n",
        "      )\n",
        "\n",
        "    current_optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
        "    current_model.compile(optimizer=current_optimizer, loss=current_NN_helper.loss_function)\n",
        "    history = current_model.fit(current_NN_helper.x_training_input, current_NN_helper.y_training_onehot,\n",
        "                        batch_size = batch_size, epochs = current_NN_helper.epochs, \n",
        "                        verbose = 1,\n",
        "                        validation_split = .1)\n",
        "\n",
        "\n",
        "    current_NN_helper.y_training_preds = np.argmax(current_model.predict(current_NN_helper.x_training_input), axis = -1)\n",
        "    current_NN_helper.y_test_preds = np.argmax(current_model.predict(current_NN_helper.x_test_input) , axis = -1)\n",
        "\n",
        "    def calc_acc(pred_vec, known_vec):\n",
        "      \n",
        "      if type(pred_vec) == type(known_vec):\n",
        "        acc = np.sum(pred_vec == known_vec, axis = 0) / len(known_vec)\n",
        "      else:\n",
        "        print('self.validate: Error! Comparison of different label encoding!')\n",
        "      \n",
        "      return acc\n",
        "\n",
        "    train_acc = calc_acc(current_NN_helper.y_training_preds, current_NN_helper.y_training_input_int)\n",
        "    test_acc = calc_acc(current_NN_helper.y_test_preds, current_NN_helper.y_test_input_int)\n",
        "\n",
        "    parameters = {\n",
        "      'test_accuracy': test_acc,\n",
        "      'option_scVI': option_scVI,\n",
        "      'n_neurons': n_neurons,\n",
        "      'n_layers': n_layers,\n",
        "      'dropout': dropout,\n",
        "      'learning_rate': learning_rate,\n",
        "      'momentum': momentum,\n",
        "      'batch_size': batch_size,\n",
        "      'batch_norm': batch_norm,\n",
        "      'l2_reg': l2_reg, \n",
        "      'leakiness_ReLU': leakiness_ReLU\n",
        "    }\n",
        "    index = len(performance_df)\n",
        "    for key, value in parameters.items():\n",
        "      performance_df.loc[index, key] = value\n",
        "\n",
        "    # Abort search if improvement in test accuracy has become insignificant\n",
        "    if False: break\n",
        "\n",
        "  return performance_df     "
      ],
      "metadata": {
        "id": "fVvfXP3xIy2J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A0eeCUgYwORc"
      },
      "outputs": [],
      "source": [
        "class Neural_Network():\n",
        "\n",
        "  def __init__(self, \n",
        "               x_input_data,\n",
        "               y_input_data,\n",
        "               len_of_output, #NEU!!!!! ÜBERGEBE LÄNGE DES OUTPUT VEKTORS WEIL Y_INPUT für feinere Knoten KEIN PANDAS DATAFRAME MEHR IST\n",
        "               z_transform_input = True,\n",
        "               list_of_hidden_layer_nodes = [30],\n",
        "               activation_function = 'relu',\n",
        "               learning_rate = 0.001,\n",
        "               momentum = .9,\n",
        "               loss_function = 'categorical_crossentropy',\n",
        "               epochs = 50):\n",
        "    \n",
        "    self.x_input_data = x_input_data\n",
        "    self.y_input_data = y_input_data\n",
        "    self.len_of_output = len_of_output\n",
        "    self.list_of_layer_nodes = [len(self.x_input_data[0])] + [nodes for nodes in list_of_hidden_layer_nodes] + [len_of_output] #[len(self.y_input_data.cat.categories)] \n",
        "    self.activation_function = activation_function \n",
        "    self.learning_rate = learning_rate\n",
        "    self.momentum = momentum\n",
        "    self.loss_function = loss_function\n",
        "    self.epochs = epochs\n",
        "    self.z_transform_input = z_transform_input\n",
        "\n",
        "    #create model\n",
        "\n",
        "    self.model = keras.models.Sequential()\n",
        "    self.process_input_data()\n",
        "    # Defer to grid_search to automatically find optimal hyperparameters for each nodes network\n",
        "    # self.grid_search()\n",
        "    for nodes, layer_idx in zip(self.list_of_layer_nodes, range(0, len(self.list_of_layer_nodes)-1)):\n",
        "\n",
        "      self.model.add(keras.layers.Dense(\n",
        "          input_shape = (nodes,),\n",
        "          units = self.list_of_layer_nodes[layer_idx+1],                            \n",
        "          kernel_initializer = 'glorot_uniform',                                          \n",
        "          bias_initializer = 'zeros',                                               \n",
        "          activation = self.activation_function if layer_idx == len(self.list_of_layer_nodes) - 1 else 'softmax'))\n",
        "\n",
        "  def process_input_data(self):\n",
        "\n",
        "    #z transformation \n",
        "\n",
        "    def z_transform_properties(data_arr):\n",
        "      '''Calculates a z transformation to center properties across cells in data_arr around mean zero'''\n",
        "      \n",
        "      mean_vals = np.mean(data_arr, axis=0)\n",
        "      std_val = np.std(data_arr)\n",
        "\n",
        "      data_transformed = (data_arr - mean_vals) / std_val\n",
        "\n",
        "      return data_transformed\n",
        "\n",
        "\n",
        "    if self.z_transform_input:\n",
        "      self.x_input_data = z_transform_properties(self.x_input_data)\n",
        "      \n",
        "    #converting labels of y_input_data to integers (mapping 'label'->int)\n",
        "\n",
        "    # ----> erstmal so vereinfacht, dass incoming y_data encoded wird -> ist wegen group_by_labels eh mit strings\n",
        "\n",
        "    print(f'network y_input before encoding: {self.y_input_data}')\n",
        "\n",
        "    self.label_encoder = LabelEncoder()\n",
        "    self.y_input_data_int = self.label_encoder.fit_transform(self.y_input_data)\n",
        "\n",
        "    # if isinstance(self.y_input_data, pd.Series):\n",
        "    #   #encode categorical labels from pandas Series as integers\n",
        "    #   self.label_encoder = LabelEncoder()\n",
        "    #   self.y_input_data_int = self.label_encoder.fit_transform(self.y_input_data.values)\n",
        "\n",
        "    # elif isinstance(self.y_input_data[0], np.ndarray) and self.y_input_data.dtype == int: #so vereinfachen, dass einfach immer encoded wird. mit neuer 'pipeline' wird eh nur y_input mit strings weitergegeben\n",
        "    #   self.y_input_data_int = self.y_input_data\n",
        "\n",
        "    # elif isinstance(self.y_input_data[0], np.ndarray) and self.y_input_data.dtype == str: #y_input kein dataframe mehr, sondern Liste wegen preprocessing!\n",
        "    #   self.y_input_data_int = self.label_encoder.fit_transform(self.y_input_data)\n",
        "\n",
        "    # else:\n",
        "\n",
        "    #   print('Error: Invalid or unknown Data Type for Y Input data')\n",
        "\n",
        "  \n",
        "\n",
        "    #split data (TO DO: implement scikit.learn cross_val_score for k-fold cross validation)\n",
        "\n",
        "    split_data_index = int(2/3 * len(self.x_input_data))\n",
        "\n",
        "    self.x_training_input = self.x_input_data[ : split_data_index]\n",
        "    self.y_training_input_int = self.y_input_data_int[ : split_data_index]\n",
        "\n",
        "    self.x_test_input = self.x_input_data[split_data_index : ]\n",
        "    self.y_test_input_int = self.y_input_data_int[split_data_index : ]\n",
        "\n",
        "    #use integer y_input data for OneHot Encoding (int -> e_i element R^(int+1)) needed for model training\n",
        "\n",
        "    self.y_training_onehot = keras.utils.to_categorical(self.y_training_input_int)\n",
        "    self.y_test_onehot = keras.utils.to_categorical(self.y_test_input_int)\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    '''Train the NN using the x_training_data input and onehot encoded y_training_onehot'''\n",
        "\n",
        "    self.optimizer = keras.optimizers.SGD(learning_rate = self.learning_rate, momentum = self.momentum)\n",
        "    \n",
        "    self.model.compile(optimizer = self.optimizer, loss = self.loss_function)\n",
        "\n",
        "    history = self.model.fit(self.x_training_input, self.y_training_onehot,\n",
        "                        batch_size = 64, epochs = self.epochs, \n",
        "                        verbose = 1,\n",
        "                        validation_split = .1)\n",
        "    \n",
        "  def predict(self, input_vec):\n",
        "    '''Calculate and return label prediction of trained model for an input vector\n",
        "        input_vec (dtype=int)'''\n",
        "\n",
        "    #returns only absolute decisions, nothing known about the difference in prediction confidence \n",
        "    pred_vec = np.argmax(self.model.predict(input_vec), axis = -1) #-1?\n",
        "\n",
        "    return pred_vec\n",
        "\n",
        "\n",
        "  def validate(self):\n",
        "\n",
        "    self.y_training_preds = self.predict(self.x_training_input)\n",
        "    self.y_test_preds = self.predict(self.x_test_input) \n",
        "\n",
        "    def calc_acc(pred_vec, known_vec):\n",
        "      \n",
        "      if type(pred_vec) == type(known_vec):\n",
        "        acc = np.sum(pred_vec == known_vec, axis = 0) / len(known_vec)\n",
        "      else:\n",
        "        print('self.validate: Error! Comparison of different label encoding!')\n",
        "      \n",
        "      return acc\n",
        "    \n",
        "    # ---> save in node memory, not in classifier model\n",
        "    # self.train_acc = calc_acc(self.y_training_preds, self.y_training_input_int) \n",
        "    # self.test_acc = calc_acc(self.y_test_preds, self.y_test_input_int)\n",
        "\n",
        "    train_acc = calc_acc(self.y_training_preds, self.y_training_input_int)\n",
        "    test_acc = calc_acc(self.y_test_preds, self.y_test_input_int)\n",
        "\n",
        "    return train_acc, test_acc\n",
        "\n",
        "\n",
        "  def master_method_NN(self, validate):\n",
        "    '''method to run all relevant neural network methods, to be called from hierarchical classifier!\n",
        "    not sure if needed'''\n",
        "    \n",
        "    # self.process_input_data()\n",
        "    # self.train()\n",
        "\n",
        "    #prediction vector soll in node_memory gespeichert werden!\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class Node_Memory\n",
        "Save Data of local Classifier in Nodes of a directed Graph, initialized by Class Hierarchical_Classifier (see below)"
      ],
      "metadata": {
        "id": "Uhf6Mz1I_-xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node_Memory():\n",
        "\n",
        "  def __init__(self, x_input_data = None, y_input_data = None, local_classifier = None, local_classifier_params = []):\n",
        "\n",
        "      #local_classifier argument is the Class of classifier (eg Neural_Network (classifier specific params, input and output needed for every possible classifier))\n",
        "      #initialize all needed attributes, but assign via HC objects\n",
        "\n",
        "      self.x_input_data = x_input_data  #processing wird von local classifier übernommen (CAVE bei Verwendung anderer LC (eg SVM), dass preprocessing einheitlich ist)\n",
        "      self.y_input_data = y_input_data \n",
        "\n",
        "      self.y_input_grouped_labels = None  #is generated via Hierarchical classifier, maps all labels to labels of children of THIS node object\n",
        "\n",
        "      self.apriori_y_input_data_indices = None #save indices of data in y_input vector, that will be used for prediction in order to compute accuracy\n",
        "      \n",
        "      #still manual splitting -> adjust to k fold CV in future\n",
        "      self.training_prediction_vec = None\n",
        "      self.test_prediction_vec = None\n",
        "\n",
        "      self.training_acc = None\n",
        "      self.test_acc = None\n",
        "\n",
        "      #SEEEEEEEHR UNSCHÖÖÖÖÖÖN, PROBLEM MIT Y_INPUT_GROUPED_BY_LABELS_BEHEBEN!! DANN INITIALISIEREN WIEDER IN KONSTRUKTOR SCHREIBEN\n",
        "      self.classifier_class = local_classifier #zwischenspeichern der Klasse bis y_input_grouped labels durch HC ausgegeben wurde --> SEHR UNSCHÖN\n",
        "      self.local_classifier_params = local_classifier_params\n",
        "\n",
        "  def initialize_local_classifier(self, local_classifier_output_len):\n",
        "    print(f'zu übergebene y_input daten: {self.y_input_grouped_labels}')\n",
        "    if self.classifier_class != None:\n",
        "      self.local_classifier = self.classifier_class(self.x_input_data, self.y_input_grouped_labels, local_classifier_output_len, *self.local_classifier_params)  #use y_input with labels that refer to the child nodes\n",
        "    else:\n",
        "      #TO DO: bessere Lösung einfallen lassen\n",
        "      print('WARNING! No classifier initialized!')"
      ],
      "metadata": {
        "id": "sZJiCqeZ0gKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class Hierarchical_Classifier \n",
        "Initialize Directed Graph with Local Classifiers in Nodes"
      ],
      "metadata": {
        "id": "bBEkqCnDAGfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_graph_from_edges(d, g, parent_key=''):\n",
        "    for key in d.keys():\n",
        "        if parent_key != '':\n",
        "            g.add_edge(parent_key, key)\n",
        "        if len(d[key]) == 0:\n",
        "            pass\n",
        "        else:\n",
        "            make_graph_from_edges(d[key], g, parent_key=key)"
      ],
      "metadata": {
        "id": "sUU8Yb1TmIUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_subgraph_nodes(g, parent_node):\n",
        "  list_of_nodes = []\n",
        "  for node in g.adj[parent_node].keys():\n",
        "    if len(g.adj[parent_node].keys()) != 0:\n",
        "      list_of_nodes.append(node)\n",
        "      list_of_nodes = list_of_nodes + list_subgraph_nodes(g, node)\n",
        "    else:\n",
        "      list_of_nodes.append(node)\n",
        "  return list_of_nodes"
      ],
      "metadata": {
        "id": "NkMWTEyXhnQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Hierarchical_Classifier():\n",
        "  '''Class connects Nodes of Local Classifiers, passes results to children classifiers and forms the final hierarchical classifier''' \n",
        "\n",
        "  def __init__(self, dict_of_cell_relations):\n",
        "    '''Params\n",
        "        - dict_of_cell_relations: used for initializing network structure of hierarchical classifier'''\n",
        "    self.dict_of_cell_relations = dict_of_cell_relations\n",
        "\n",
        "\n",
        "  def make_classifier_graph(self):\n",
        "    '''Compute Graph from a given dictionary of cell relationships'''\n",
        "    self.graph = nx.DiGraph()\n",
        "    make_graph_from_edges(self.dict_of_cell_relations, self.graph)\n",
        "\n",
        "  def init_node_memory_object(self, node, memory_class_params=[]):\n",
        "    '''Add memory object to each node; memory object organizes all relevant local classifier params'''\n",
        "    self.graph.add_node(node, memory=Node_Memory(*memory_class_params)) #lade die relevanten Daten\n",
        "    if len(memory_class_params) != 0: #auch sehr unschön ---> so ändern, dass konflit mit NN Initialisierung und berechnen von y_input_grouped_labels in node memory nicht auftritt\n",
        "      self.group_labels_of_subgraph_to_parent_label(node) #schreibe die relevanten (komprimierten) y_input daten durch aufrufen der methode\n",
        "      \n",
        "      print(f'Aus init_node_memory_object von Knoten {node}: output größe initialisiert als {len(self.graph.adj[node].keys())}')\n",
        "      \n",
        "      self.graph.nodes[node]['memory'].initialize_local_classifier(len(self.graph.adj[node].keys())) #initialisiere den local_classifier (jetzt ist y_input_grouped_labels wegen vorherigerm Methodenaufruf in node memory gegeben)\n",
        "                                                                                                      #GEÄNDERT: ÜBERGEBE LÄNGE DES LETZTEN LAYERS DES NN, vorher aus cat.categories geschlossen\n",
        "                                                                                                      #aber y_input_grouped_labels ist eine Liste\n",
        "  def run_local_classifier(self, node):\n",
        "    '''DEV METHOD: use for calling single methods at once to begin with'''\n",
        "    \n",
        "    #wo werden dem ersten local classifier die daten übergeben? --> aktuell manuell (s.u. TESTEN)\n",
        "\n",
        "    #ONLY NEEDED FOR GLOBAL ACCURACY\n",
        "    #save indices of known labels of daughter nodes in these corresponding nodes (i.e. eg node 'TNK' knows indices of 'TNK' in y_input_data of node 'all' ) \n",
        "    next_labels = [label for label in self.graph[node].keys()]\n",
        "    for label in next_labels:\n",
        "      self.graph.nodes[label]['memory'].apriori_y_input_data_indices = np.where(np.array(self.graph.nodes[node]['memory'].y_input_data) == label)\n",
        "\n",
        "    #run needed local classifier methods to train model\n",
        "    self.graph.nodes[node]['memory'].local_classifier.process_input_data()\n",
        "    self.graph.nodes[node]['memory'].local_classifier.train()\n",
        "\n",
        "    #predict trainings and test data (still splitted manually -> k fold cross validation) and save in corresponding node memory\n",
        "    self.graph.nodes[node]['memory'].training_prediction_vec = self.graph.nodes[node]['memory'].local_classifier.predict(self.graph.nodes[node]['memory'].local_classifier.x_training_input)\n",
        "    self.graph.nodes[node]['memory'].test_prediction_vec = self.graph.nodes[node]['memory'].local_classifier.predict(self.graph.nodes[node]['memory'].local_classifier.x_test_input)\n",
        "\n",
        "    #validate model \n",
        "    self.graph.nodes[node]['memory'].training_acc, self.graph.nodes[node]['memory'].test_acc =  self.graph.nodes[node]['memory'].local_classifier.validate()\n",
        "\n",
        "\n",
        "  def run_local_classifier_master(self, node, validate = True):\n",
        "    '''run master method of local classifier'''\n",
        "    pass\n",
        "\n",
        "\n",
        "  #automatisches mapping der feineren labels zu den groberen durch iterieren über die Labels der subgraphen, ausführen über \n",
        "  #Hierarchical_Classifier in jedem Knoten --> als y_input_data in node_memory speichern\n",
        "  #Ziel: mapping automatisieren, dafür: nimm 'Nachbarn' als nächste parentlabel, deren kinder müssen \n",
        "  #dann zu Nachbarn gemapped werden\n",
        "\n",
        "  #benutze den graph, weil hier die ecken bereits klar sind, klarer als in gegebenem dictionary (dort müsste über keys von keys iteriert werden)\n",
        "\n",
        "  def group_labels_of_subgraph_to_parent_label(self, super_node):\n",
        "    '''Maps y_input_data labels to parent label (eg map (CD4 T, CD8 T, NK) -> (TNK) for all neighbors of given super_node (eg TNK, B, Others) of Graph g\n",
        "      Params:\n",
        "        - super_node: node at which local classifier has to be run\n",
        "        \n",
        "      Saves result in y_input_grouped_labels attribute of Node_memory of super_node''' \n",
        "\n",
        "    mapping_dict = {node : [child_node for child_node in list_subgraph_nodes(self.graph, node)] for node in self.graph.adj[super_node].keys() }\n",
        "\n",
        "    #check if some neighbor nodes have already achieved highest annotation depth (terminal nodes) and map those to itself\n",
        "    for key in mapping_dict.keys():\n",
        "      if mapping_dict[key] == []:\n",
        "        mapping_dict[key] = key\n",
        "\n",
        "    mapper = {k:v for v,k in pd.Series(mapping_dict).explode().iteritems()} #write dict such that key and value are interchanged, then map; von StackOverFlow: letze antwort https://stackoverflow.com/questions/32262982/pandas-combining-multiple-categories-into-one\n",
        "    \n",
        "\n",
        "    #check if some cells were mislead and throw them out (otherwise they are labelled nan which gives more output units for neural network than needed + might be problematic for training of NN if \n",
        "    #cells are used asa data which were initially wrong, but are mapped to an output category)\n",
        "    #DOES NOT WORK FOR PREDICTION OF UNLABELED CELLS OF COURSE! OPTION TRAINING/PREDICTION MODE NEEDED!\n",
        "\n",
        "    idx_of_incorrectly_labelled_cells = []\n",
        "    for cell_label, cell_idx in zip(self.graph.nodes[super_node]['memory'].y_input_data, range(0, len(self.graph.nodes[super_node]['memory'].y_input_data))):\n",
        "      if cell_label not in mapper.keys():\n",
        "        idx_of_incorrectly_labelled_cells.append(cell_idx)\n",
        "    self.graph.nodes[super_node]['memory'].y_input_data = np.delete(self.graph.nodes[super_node]['memory'].y_input_data,idx_of_incorrectly_labelled_cells, axis = 0)\n",
        "    self.graph.nodes[super_node]['memory'].x_input_data = np.delete(self.graph.nodes[super_node]['memory'].x_input_data,idx_of_incorrectly_labelled_cells, axis = 0)\n",
        "    \n",
        "    self.graph.nodes[super_node]['memory'].y_input_grouped_labels = [mapper.get(k) for k in self.graph.nodes[super_node]['memory'].y_input_data]\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "  def subset_pred_vec(self, node):\n",
        "    next_labels = [label for label in self.graph[node].keys()]\n",
        "\n",
        "    #fuse training and test data predictions to one prediction in order to have more data for next node (use local variable)\n",
        "    current_pred_vec = np.concatenate((self.graph.nodes[node]['memory'].training_prediction_vec, self.graph.nodes[node]['memory'].test_prediction_vec), axis = 0)\n",
        "    current_pred_vec = self.graph.nodes[node]['memory'].local_classifier.label_encoder.inverse_transform(current_pred_vec)\n",
        "\n",
        "    for next_label in next_labels:\n",
        "\n",
        "      #following line decodes integer output of neural network to string labels (for test and training data -> prediction vecs now overwritten, might be appropriate to use local variable) in order to compare with names of following nodes (again encoded by following local classifier)\n",
        "      # self.graph.nodes[node]['memory'].training_prediction_vec = self.graph.nodes[node]['memory'].local_classifier.label_encoder.inverse_transform(self.graph.nodes[node]['memory'].training_prediction_vec)\n",
        "      # self.graph.nodes[node]['memory'].test_prediction_vec = self.graph.nodes[node]['memory'].local_classifier.label_encoder.inverse_transform(self.graph.nodes[node]['memory'].test_prediction_vec)\n",
        "      # temp_idx_vec = np.where(np.array(self.graph.nodes[node]['memory'].prediction_vec) == next_label)\n",
        "      \n",
        "      #hier nochmal über auswirkung auf 'global accuracy' nachdenken -> falsch gelabelte zellen werden in group_labels_of_subgraph_to_parent_label\n",
        "      #des entsprechenden classifiers/knoten herausgeworfen\n",
        "      temp_idx_vec = np.where(current_pred_vec == next_label) \n",
        "      \n",
        "      self.graph.nodes[next_label]['memory'].x_input_data = np.array(self.graph.nodes[node]['memory'].x_input_data)[temp_idx_vec]\n",
        "      self.graph.nodes[next_label]['memory'].y_input_data = np.array(self.graph.nodes[node]['memory'].y_input_data)[temp_idx_vec]\n",
        "\n",
        "\n",
        "  def master_method(self):\n",
        "    '''method to run all relevant methods -> \"one-click\" initializing of hierarchical classifier (?)'''\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "IQaUTDTS_X1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test of Hierarchical Classifier "
      ],
      "metadata": {
        "id": "F_RdxaiARRj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-misc"
      ],
      "metadata": {
        "id": "NFbEfhcHaN9_",
        "outputId": "35aedd33-e6e8-40c6-94fe-286376462823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-misc\n",
            "  Downloading scikit_misc-0.1.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scikit-misc) (1.21.5)\n",
            "Installing collected packages: scikit-misc\n",
            "Successfully installed scikit-misc-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scvi-tools"
      ],
      "metadata": {
        "id": "_uCEowkMaXGA",
        "outputId": "4432da5c-c0fa-4923-aeba-a20e18cf18d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scvi-tools\n",
            "  Downloading scvi_tools-0.15.2-py3-none-any.whl (260 kB)\n",
            "\u001b[K     |████████████████████████████████| 260 kB 22.1 MB/s \n",
            "\u001b[?25hCollecting rich>=9.1.0\n",
            "  Downloading rich-12.0.0-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools) (1.3.5)\n",
            "Collecting importlib-metadata<2.0,>=1.0\n",
            "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting anndata>=0.7.5\n",
            "  Downloading anndata-0.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting docrep>=0.3.2\n",
            "  Downloading docrep-0.3.2.tar.gz (33 kB)\n",
            "Collecting pyro-ppl>=1.6.0\n",
            "  Downloading pyro_ppl-1.8.0-py3-none-any.whl (713 kB)\n",
            "\u001b[K     |████████████████████████████████| 713 kB 57.6 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning<1.6,>=1.5\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools) (1.21.5)\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.1-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools) (3.1.0)\n",
            "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.7/dist-packages (from scvi-tools) (0.3.1)\n",
            "Collecting numpyro\n",
            "  Downloading numpyro-0.9.1-py3-none-any.whl (283 kB)\n",
            "\u001b[K     |████████████████████████████████| 283 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.6.0\n",
            "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.56.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools) (4.63.0)\n",
            "Requirement already satisfied: openpyxl>=3.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools) (3.0.9)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from scvi-tools) (7.6.5)\n",
            "Collecting flax\n",
            "  Downloading flax-0.4.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.7/dist-packages (from scvi-tools) (1.0.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.5->scvi-tools) (3.10.0.2)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.5->scvi-tools) (5.5.0)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.5->scvi-tools) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.5->scvi-tools) (21.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from docrep>=0.3.2->scvi-tools) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->scvi-tools) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<2.0,>=1.0->scvi-tools) (3.7.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->scvi-tools) (3.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->scvi-tools) (1.0.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0->scvi-tools) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20->anndata>=0.7.5->scvi-tools) (3.0.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->scvi-tools) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->scvi-tools) (2.8.2)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 41.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<1.6,>=1.5->scvi-tools) (2.8.0)\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 41.8 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.7 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=9.1.0->scvi-tools) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->scvi-tools) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->scvi-tools) (3.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (1.44.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (1.3.1)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (3.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 47.2 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<1.6,>=1.5->scvi-tools) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax->scvi-tools) (1.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax->scvi-tools) (3.2.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools) (3.5.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools) (4.10.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools) (1.0.2)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools) (5.1.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools) (5.1.3)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools) (5.3.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools) (1.0.18)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->scvi-tools) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->scvi-tools) (4.9.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->scvi-tools) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->scvi-tools) (0.18.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets->scvi-tools) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->scvi-tools) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax->scvi-tools) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax->scvi-tools) (0.11.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->scvi-tools) (0.5.1)\n",
            "Collecting multipledispatch\n",
            "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: jaxlib>=0.1.65 in /usr/local/lib/python3.7/dist-packages (from numpyro->scvi-tools) (0.3.0+cuda11.cudnn805)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.65->numpyro->scvi-tools) (2.0)\n",
            "Collecting chex>=0.0.4\n",
            "  Downloading chex-0.1.1-py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->scvi-tools) (0.11.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->scvi-tools) (0.1.6)\n",
            "Building wheels for collected packages: docrep, future\n",
            "  Building wheel for docrep (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docrep: filename=docrep-0.3.2-py3-none-any.whl size=19896 sha256=b5473768949b86e00764142cd01f39f80a79703f9b75376790d7c1d2d38d7ec8\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/a1/89/8c863c13903012831ee9e6f0544375e06de9c461659e968c40\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=7f28ed801ee1d3a6402756054f0b683441013813faa84cb83ed1ff27c47f59e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built docrep future\n",
            "Installing collected packages: importlib-metadata, setuptools, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, markdown, fsspec, chex, aiohttp, torchmetrics, PyYAML, pyro-api, optax, multipledispatch, future, commonmark, rich, pytorch-lightning, pyro-ppl, numpyro, flax, docrep, anndata, scvi-tools\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.2\n",
            "    Uninstalling importlib-metadata-4.11.2:\n",
            "      Successfully uninstalled importlib-metadata-4.11.2\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.3.6\n",
            "    Uninstalling Markdown-3.3.6:\n",
            "      Successfully uninstalled Markdown-3.3.6\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 anndata-0.8.0 async-timeout-4.0.2 asynctest-0.13.0 chex-0.1.1 commonmark-0.9.1 docrep-0.3.2 flax-0.4.0 frozenlist-1.3.0 fsspec-2022.2.0 future-0.18.2 importlib-metadata-1.7.0 markdown-3.3.4 multidict-6.0.2 multipledispatch-0.6.0 numpyro-0.9.1 optax-0.1.1 pyDeprecate-0.3.1 pyro-api-0.1.2 pyro-ppl-1.8.0 pytorch-lightning-1.5.10 rich-12.0.0 scvi-tools-0.15.2 setuptools-59.5.0 torchmetrics-0.7.2 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy \n"
      ],
      "metadata": {
        "id": "ICWY1MyZRVbk",
        "outputId": "2bbf1138-c9c5-4999-f2b4-5f93b926c46b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.8.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |▎                               | 20 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |▌                               | 30 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 40 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |▉                               | 51 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 61 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92 kB 17.1 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 112 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 122 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 133 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 163 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 184 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 194 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 204 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 215 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 225 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 235 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 245 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 256 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 266 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 276 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 286 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 296 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 307 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 317 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 327 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 337 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 348 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 358 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 368 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 378 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 389 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 399 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 409 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 419 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 430 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 440 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 450 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 460 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 471 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 481 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 491 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 501 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 512 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 522 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 532 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 542 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 552 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 563 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 573 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 583 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 593 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 604 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 614 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 624 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 634 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 645 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 655 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 665 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 675 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 686 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 696 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 706 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 716 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 727 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 737 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 747 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 757 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 768 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 778 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 788 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 798 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 808 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 819 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 829 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 839 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 849 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 860 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 870 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 880 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 890 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 901 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 911 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 921 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 931 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 942 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 952 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 962 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 972 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 983 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 993 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.1 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.2 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.3 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.3 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.3 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.3 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.5 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.6 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.7 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.7 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.7 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.7 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.7 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.7 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.8 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.9 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0 MB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0 MB 18.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.10.2)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.51.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.2)\n",
            "Collecting sinfo\n",
            "  Downloading sinfo-0.3.4.tar.gz (24 kB)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.21.5)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.7.0)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.63.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.0)\n",
            "Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.7.0)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.1.0)\n",
            "Requirement already satisfied: anndata>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy) (21.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->scanpy) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (59.5.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->scanpy) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 37.8 MB/s \n",
            "\u001b[?25hCollecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables->scanpy) (2.8.1)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, sinfo\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=27f9b1a8ed4fda12eb8fc8927c25bd7b2c69f090dabfa99afb3e16a2c93781c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=a8585c271dd6fab2b41f7ba2afa582a4401b2ddac7233fc939b07b120d983131\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "  Building wheel for sinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sinfo: filename=sinfo-0.3.4-py3-none-any.whl size=7899 sha256=e69dbbcf5fc13ff17272db2f4d316e5f812dd5ca917ea764abe70d6d1ed72889\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/ca/56/344d532fe53e855ccd6549795d370588ab8123907eecf4cf30\n",
            "Successfully built umap-learn pynndescent sinfo\n",
            "Installing collected packages: stdlib-list, pynndescent, umap-learn, sinfo, scanpy\n",
            "Successfully installed pynndescent-0.5.6 scanpy-1.8.2 sinfo-0.3.4 stdlib-list-0.8.0 umap-learn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc"
      ],
      "metadata": {
        "id": "qvbTHN7Tek7N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yExXmrwfQuPx",
        "outputId": "b7d09bf4-abff-4ef6-df6e-80014ac5e895"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adata = sc.read('drive/MyDrive/TNK_Zhang_BRCA_labeled_Test.h5ad')\n",
        "adata = adata[adata.obs['Level_4'] != '']\n",
        "scvi.model.SCVI.setup_anndata(adata, batch_key=\"batch\")\n",
        "vae = scvi.model.SCVI(adata)\n",
        "vae.train(max_epochs=100)\n",
        "adata.obsm['X_scvi'] = vae.get_latent_representation()\n",
        "#adata = adata[adata.obs.sample(n=5000, replace=False, random_state=1, axis=0).index,:].copy()\n",
        "#sc.pp.highly_variable_genes(adata, n_top_genes=3000, flavor='seurat_v3', subset=True)"
      ],
      "metadata": {
        "id": "G51DK4iQaeli"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-p3UEGsAtVAY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_nn = Neural_Network(\n",
        "    adata.obsm['X_scVI'],\n",
        "    adata.obs['Level_4'],\n",
        "    5)"
      ],
      "metadata": {
        "id": "C95TETNFtF-f",
        "outputId": "d7fd1dd7-c983-400f-d0f8-a99ad593a272",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "network y_input before encoding: CATTATCCACAGCGTC.Post_P013_b    ab_T\n",
            "ATCATGGCAAGGTGTG.Post_P023_b    ab_T\n",
            "CGTGTAAAGACAGACC.Pre_P013_b     ab_T\n",
            "TTGGAACCATCGATTG.Pre_P014_b       NK\n",
            "AGCTCTCTCTCACATT.Post_P018_b    ab_T\n",
            "                                ... \n",
            "AGAGTGGGTACGACCC.Prog_P013_b      NK\n",
            "CAAGATCTCAACGGCC.Pre_P008_b     ab_T\n",
            "GAGGTGAAGGCCCTCA.Pre_P014_b     ab_T\n",
            "CCATGTCCAGCCACCA.Post_P017_b    ab_T\n",
            "ATCGAGTTCATACGGT.Post_P002_b    ab_T\n",
            "Name: Level_4, Length: 4975, dtype: category\n",
            "Categories (5, object): ['ILC', 'MAIT', 'NK', 'ab_T', 'gd_T']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "performance_df = test_nn.grid_search(random_tries=20)"
      ],
      "metadata": {
        "id": "ql7m3Puj-Kmm",
        "outputId": "6dd14e04-6301-4810-80fe-bbef6bb23e35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "50/50 [==============================] - 2s 22ms/step - loss: 1.3175 - val_loss: 0.8488\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.7141 - val_loss: 0.5873\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.5815 - val_loss: 0.5140\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.5205 - val_loss: 0.4686\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.4787 - val_loss: 0.4348\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.4464 - val_loss: 0.4143\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.4207 - val_loss: 0.3887\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3956 - val_loss: 0.3702\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3768 - val_loss: 0.3486\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3582 - val_loss: 0.3528\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3435 - val_loss: 0.3243\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3296 - val_loss: 0.3189\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3219 - val_loss: 0.3150\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3100 - val_loss: 0.2990\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2978 - val_loss: 0.2905\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2923 - val_loss: 0.2873\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2844 - val_loss: 0.2831\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2793 - val_loss: 0.2824\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 0s 10ms/step - loss: 0.2759 - val_loss: 0.2873\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2678 - val_loss: 0.2738\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2641 - val_loss: 0.2660\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2589 - val_loss: 0.2627\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2555 - val_loss: 0.2800\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2556 - val_loss: 0.2567\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2497 - val_loss: 0.2711\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2479 - val_loss: 0.2530\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2414 - val_loss: 0.2480\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2388 - val_loss: 0.2568\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2420 - val_loss: 0.2484\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2411 - val_loss: 0.2449\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2281 - val_loss: 0.2488\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2257 - val_loss: 0.2600\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2270 - val_loss: 0.2431\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2272 - val_loss: 0.2479\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2214 - val_loss: 0.2389\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2170 - val_loss: 0.2460\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2130 - val_loss: 0.2354\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2138 - val_loss: 0.2490\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2121 - val_loss: 0.2306\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2089 - val_loss: 0.2294\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2065 - val_loss: 0.2308\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2083 - val_loss: 0.2330\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2026 - val_loss: 0.2240\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2041 - val_loss: 0.2239\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2042 - val_loss: 0.2910\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2124 - val_loss: 0.2332\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 0.2317\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1978 - val_loss: 0.2423\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1921 - val_loss: 0.2178\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1902 - val_loss: 0.2223\n",
            "Epoch 1/50\n",
            "50/50 [==============================] - 1s 5ms/step - loss: 1.4187 - val_loss: 1.0437\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.8635 - val_loss: 0.7118\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.6533 - val_loss: 0.5642\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.5475 - val_loss: 0.4781\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.4834 - val_loss: 0.4237\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.4404 - val_loss: 0.3866\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.4091 - val_loss: 0.3583\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.3846 - val_loss: 0.3364\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3644 - val_loss: 0.3185\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.3475 - val_loss: 0.3039\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3329 - val_loss: 0.2913\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.3200 - val_loss: 0.2807\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.3089 - val_loss: 0.2717\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2989 - val_loss: 0.2640\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2901 - val_loss: 0.2570\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2823 - val_loss: 0.2504\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2752 - val_loss: 0.2451\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2690 - val_loss: 0.2401\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2632 - val_loss: 0.2360\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2580 - val_loss: 0.2319\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2534 - val_loss: 0.2283\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2489 - val_loss: 0.2251\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2449 - val_loss: 0.2222\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2412 - val_loss: 0.2195\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2379 - val_loss: 0.2163\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2345 - val_loss: 0.2143\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2315 - val_loss: 0.2124\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2290 - val_loss: 0.2104\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2262 - val_loss: 0.2084\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2237 - val_loss: 0.2060\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2212 - val_loss: 0.2044\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2190 - val_loss: 0.2027\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2169 - val_loss: 0.2015\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2149 - val_loss: 0.1996\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2131 - val_loss: 0.1983\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2112 - val_loss: 0.1970\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2096 - val_loss: 0.1960\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2080 - val_loss: 0.1947\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2064 - val_loss: 0.1937\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2048 - val_loss: 0.1919\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2034 - val_loss: 0.1913\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2022 - val_loss: 0.1899\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2008 - val_loss: 0.1885\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1996 - val_loss: 0.1886\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.1981 - val_loss: 0.1866\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.1971 - val_loss: 0.1862\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1959 - val_loss: 0.1848\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.1949 - val_loss: 0.1840\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.1939 - val_loss: 0.1834\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1929 - val_loss: 0.1823\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 1.0466 - val_loss: 0.6595\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.6106 - val_loss: 0.5394\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.5362 - val_loss: 0.4910\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4953 - val_loss: 0.4599\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4670 - val_loss: 0.4372\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4452 - val_loss: 0.4188\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.4276 - val_loss: 0.4037\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4123 - val_loss: 0.3914\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.3990 - val_loss: 0.3784\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3876 - val_loss: 0.3685\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.3773 - val_loss: 0.3600\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3682 - val_loss: 0.3498\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.3593 - val_loss: 0.3428\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.3512 - val_loss: 0.3344\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3445 - val_loss: 0.3288\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3377 - val_loss: 0.3228\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3315 - val_loss: 0.3154\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3258 - val_loss: 0.3095\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3204 - val_loss: 0.3057\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3153 - val_loss: 0.2995\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3108 - val_loss: 0.2965\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.3062 - val_loss: 0.2936\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3019 - val_loss: 0.2881\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2980 - val_loss: 0.2857\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2943 - val_loss: 0.2805\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 0.2785\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2875 - val_loss: 0.2743\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2843 - val_loss: 0.2707\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2813 - val_loss: 0.2701\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2780 - val_loss: 0.2681\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2755 - val_loss: 0.2668\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2729 - val_loss: 0.2624\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2702 - val_loss: 0.2574\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2680 - val_loss: 0.2568\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2653 - val_loss: 0.2563\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2633 - val_loss: 0.2561\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2611 - val_loss: 0.2496\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2592 - val_loss: 0.2485\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.2573 - val_loss: 0.2454\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.2552 - val_loss: 0.2435\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2537 - val_loss: 0.2447\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2520 - val_loss: 0.2425\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2504 - val_loss: 0.2411\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2489 - val_loss: 0.2413\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2473 - val_loss: 0.2373\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2458 - val_loss: 0.2387\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2443 - val_loss: 0.2354\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2427 - val_loss: 0.2368\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2418 - val_loss: 0.2351\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2399 - val_loss: 0.2333\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 2s 7ms/step - loss: 1.5164 - val_loss: 1.4301\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.3449 - val_loss: 1.2722\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.2000 - val_loss: 1.1467\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.0906 - val_loss: 1.0565\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.0163 - val_loss: 0.9942\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.9656 - val_loss: 0.9469\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.9259 - val_loss: 0.9060\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.8897 - val_loss: 0.8655\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.8520 - val_loss: 0.8212\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.8088 - val_loss: 0.7699\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.7597 - val_loss: 0.7148\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.7102 - val_loss: 0.6626\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.6650 - val_loss: 0.6154\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.6252 - val_loss: 0.5739\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.5900 - val_loss: 0.5376\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.5592 - val_loss: 0.5063\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.5325 - val_loss: 0.4812\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.5093 - val_loss: 0.4567\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4892 - val_loss: 0.4368\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4711 - val_loss: 0.4199\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4545 - val_loss: 0.4029\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4391 - val_loss: 0.3879\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4246 - val_loss: 0.3785\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4114 - val_loss: 0.3626\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3988 - val_loss: 0.3523\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3869 - val_loss: 0.3381\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3764 - val_loss: 0.3323\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3661 - val_loss: 0.3179\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3570 - val_loss: 0.3090\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3482 - val_loss: 0.3015\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3400 - val_loss: 0.2964\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3325 - val_loss: 0.2840\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3254 - val_loss: 0.2786\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3186 - val_loss: 0.2793\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3126 - val_loss: 0.2691\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3071 - val_loss: 0.2673\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3026 - val_loss: 0.2601\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2974 - val_loss: 0.2522\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2937 - val_loss: 0.2591\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2894 - val_loss: 0.2519\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2859 - val_loss: 0.2448\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2823 - val_loss: 0.2422\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2784 - val_loss: 0.2373\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2763 - val_loss: 0.2431\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2732 - val_loss: 0.2340\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2705 - val_loss: 0.2311\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2675 - val_loss: 0.2434\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2658 - val_loss: 0.2350\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2630 - val_loss: 0.2329\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2602 - val_loss: 0.2232\n",
            "Epoch 1/50\n",
            "50/50 [==============================] - 1s 5ms/step - loss: 1.3726 - val_loss: 0.9947\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.7930 - val_loss: 0.6218\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.5624 - val_loss: 0.4663\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.4634 - val_loss: 0.3913\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.4121 - val_loss: 0.3496\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3796 - val_loss: 0.3225\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.3557 - val_loss: 0.3029\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.3369 - val_loss: 0.2874\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.3215 - val_loss: 0.2753\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3082 - val_loss: 0.2642\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2967 - val_loss: 0.2556\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2869 - val_loss: 0.2475\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2782 - val_loss: 0.2412\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2706 - val_loss: 0.2349\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2639 - val_loss: 0.2298\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2582 - val_loss: 0.2253\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2530 - val_loss: 0.2216\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2485 - val_loss: 0.2176\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2445 - val_loss: 0.2146\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2408 - val_loss: 0.2118\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 0.2092\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2344 - val_loss: 0.2069\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2318 - val_loss: 0.2045\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2293 - val_loss: 0.2032\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2270 - val_loss: 0.2005\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2247 - val_loss: 0.1992\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2228 - val_loss: 0.1979\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2210 - val_loss: 0.1969\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2191 - val_loss: 0.1951\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2176 - val_loss: 0.1938\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2162 - val_loss: 0.1927\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2146 - val_loss: 0.1909\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2132 - val_loss: 0.1901\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2120 - val_loss: 0.1894\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2107 - val_loss: 0.1877\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2096 - val_loss: 0.1873\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2084 - val_loss: 0.1862\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2073 - val_loss: 0.1851\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2063 - val_loss: 0.1853\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2052 - val_loss: 0.1835\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2042 - val_loss: 0.1830\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2032 - val_loss: 0.1824\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2023 - val_loss: 0.1810\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2015 - val_loss: 0.1805\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.2006 - val_loss: 0.1795\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.1997 - val_loss: 0.1793\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1990 - val_loss: 0.1787\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1982 - val_loss: 0.1772\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.1973 - val_loss: 0.1774\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 0s 2ms/step - loss: 0.1966 - val_loss: 0.1764\n",
            "Epoch 1/50\n",
            "75/75 [==============================] - 1s 6ms/step - loss: 1.5589 - val_loss: 1.2948\n",
            "Epoch 2/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 1.1171 - val_loss: 0.9807\n",
            "Epoch 3/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.9038 - val_loss: 0.8088\n",
            "Epoch 4/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.7792 - val_loss: 0.7010\n",
            "Epoch 5/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.7010 - val_loss: 0.6364\n",
            "Epoch 6/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.6511 - val_loss: 0.5954\n",
            "Epoch 7/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.6157 - val_loss: 0.5662\n",
            "Epoch 8/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.5887 - val_loss: 0.5437\n",
            "Epoch 9/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.5667 - val_loss: 0.5247\n",
            "Epoch 10/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.5480 - val_loss: 0.5087\n",
            "Epoch 11/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.5315 - val_loss: 0.4950\n",
            "Epoch 12/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.5171 - val_loss: 0.4825\n",
            "Epoch 13/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.5040 - val_loss: 0.4702\n",
            "Epoch 14/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4925 - val_loss: 0.4591\n",
            "Epoch 15/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4814 - val_loss: 0.4485\n",
            "Epoch 16/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4713 - val_loss: 0.4405\n",
            "Epoch 17/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4619 - val_loss: 0.4310\n",
            "Epoch 18/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4530 - val_loss: 0.4253\n",
            "Epoch 19/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4447 - val_loss: 0.4158\n",
            "Epoch 20/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4367 - val_loss: 0.4093\n",
            "Epoch 21/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4292 - val_loss: 0.4053\n",
            "Epoch 22/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4222 - val_loss: 0.3982\n",
            "Epoch 23/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4155 - val_loss: 0.3905\n",
            "Epoch 24/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4089 - val_loss: 0.3847\n",
            "Epoch 25/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.4024 - val_loss: 0.3803\n",
            "Epoch 26/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3966 - val_loss: 0.3780\n",
            "Epoch 27/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3908 - val_loss: 0.3725\n",
            "Epoch 28/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3852 - val_loss: 0.3659\n",
            "Epoch 29/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3795 - val_loss: 0.3617\n",
            "Epoch 30/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3744 - val_loss: 0.3588\n",
            "Epoch 31/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3691 - val_loss: 0.3532\n",
            "Epoch 32/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3635 - val_loss: 0.3456\n",
            "Epoch 33/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3588 - val_loss: 0.3450\n",
            "Epoch 34/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3539 - val_loss: 0.3430\n",
            "Epoch 35/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3485 - val_loss: 0.3340\n",
            "Epoch 36/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3442 - val_loss: 0.3322\n",
            "Epoch 37/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3395 - val_loss: 0.3329\n",
            "Epoch 38/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3353 - val_loss: 0.3262\n",
            "Epoch 39/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3309 - val_loss: 0.3219\n",
            "Epoch 40/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3272 - val_loss: 0.3186\n",
            "Epoch 41/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3228 - val_loss: 0.3126\n",
            "Epoch 42/50\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3194 - val_loss: 0.3119\n",
            "Epoch 43/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3156 - val_loss: 0.3064\n",
            "Epoch 44/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3122 - val_loss: 0.3037\n",
            "Epoch 45/50\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3087 - val_loss: 0.3033\n",
            "Epoch 46/50\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.3057 - val_loss: 0.3018\n",
            "Epoch 47/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3028 - val_loss: 0.3007\n",
            "Epoch 48/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.2996 - val_loss: 0.2994\n",
            "Epoch 49/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.2970 - val_loss: 0.2982\n",
            "Epoch 50/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.2941 - val_loss: 0.2926\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.9347 - val_loss: 0.5481\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4582 - val_loss: 0.3387\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3558 - val_loss: 0.2911\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3177 - val_loss: 0.2698\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2939 - val_loss: 0.2514\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2752 - val_loss: 0.2364\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2585 - val_loss: 0.2268\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2426 - val_loss: 0.2133\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2288 - val_loss: 0.2019\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2153 - val_loss: 0.1885\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2059 - val_loss: 0.1815\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1960 - val_loss: 0.1751\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1887 - val_loss: 0.1726\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1833 - val_loss: 0.1675\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.1783 - val_loss: 0.1588\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.1741 - val_loss: 0.1603\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1702 - val_loss: 0.1581\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.1670 - val_loss: 0.1536\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.1642 - val_loss: 0.1580\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1623 - val_loss: 0.1572\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1582 - val_loss: 0.1509\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1564 - val_loss: 0.1452\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1542 - val_loss: 0.1664\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.1539 - val_loss: 0.1448\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.1501 - val_loss: 0.1453\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1487 - val_loss: 0.1485\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.1465 - val_loss: 0.1391\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.1450 - val_loss: 0.1430\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.1445 - val_loss: 0.1468\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.1426 - val_loss: 0.1430\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.1403 - val_loss: 0.1426\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.1391 - val_loss: 0.1479\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 1s 8ms/step - loss: 0.1369 - val_loss: 0.1395\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1356 - val_loss: 0.1435\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.1342 - val_loss: 0.1612\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1330 - val_loss: 0.1371\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.1326 - val_loss: 0.1398\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.1300 - val_loss: 0.1433\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.1292 - val_loss: 0.1325\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1281 - val_loss: 0.1362\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1261 - val_loss: 0.1552\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1241 - val_loss: 0.1357\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.1227 - val_loss: 0.1362\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1224 - val_loss: 0.1462\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1203 - val_loss: 0.1342\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.1193 - val_loss: 0.1391\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1174 - val_loss: 0.1335\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1180 - val_loss: 0.1427\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 0.1335\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1140 - val_loss: 0.1334\n",
            "Epoch 1/50\n",
            "50/50 [==============================] - 2s 20ms/step - loss: 1.5048 - val_loss: 1.3655\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 1.2407 - val_loss: 1.1292\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 1.0422 - val_loss: 0.9812\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.9404 - val_loss: 0.9070\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.8834 - val_loss: 0.8468\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.8215 - val_loss: 0.7655\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.7316 - val_loss: 0.6585\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.6267 - val_loss: 0.5434\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.5306 - val_loss: 0.4470\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.4557 - val_loss: 0.3812\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 1s 18ms/step - loss: 0.4072 - val_loss: 0.3419\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.3743 - val_loss: 0.3169\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.3528 - val_loss: 0.2985\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.3367 - val_loss: 0.2898\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.3233 - val_loss: 0.2829\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.3145 - val_loss: 0.2726\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.3065 - val_loss: 0.2611\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.2984 - val_loss: 0.2619\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2915 - val_loss: 0.2584\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2839 - val_loss: 0.2426\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2781 - val_loss: 0.2394\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2716 - val_loss: 0.2317\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2660 - val_loss: 0.2348\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2619 - val_loss: 0.2286\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2558 - val_loss: 0.2243\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.2504 - val_loss: 0.2155\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2452 - val_loss: 0.2197\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2416 - val_loss: 0.2178\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.2376 - val_loss: 0.2090\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.2336 - val_loss: 0.2146\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2269 - val_loss: 0.2020\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2193 - val_loss: 0.1989\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.2129 - val_loss: 0.2021\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.2089 - val_loss: 0.1918\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.2065 - val_loss: 0.2032\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1979 - val_loss: 0.1819\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1941 - val_loss: 0.1826\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1890 - val_loss: 0.1793\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.1855 - val_loss: 0.1778\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1817 - val_loss: 0.1880\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 1s 16ms/step - loss: 0.1815 - val_loss: 0.1902\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1744 - val_loss: 0.1764\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1700 - val_loss: 0.1876\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1700 - val_loss: 0.1874\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1630 - val_loss: 0.1698\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 1s 18ms/step - loss: 0.1617 - val_loss: 0.1897\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1591 - val_loss: 0.1731\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1567 - val_loss: 0.1913\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1530 - val_loss: 0.1642\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 1s 17ms/step - loss: 0.1508 - val_loss: 0.1719\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 1.0857 - val_loss: 0.7308\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.6629 - val_loss: 0.5807\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5724 - val_loss: 0.5211\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 0.4845\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4924 - val_loss: 0.4590\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4679 - val_loss: 0.4389\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4477 - val_loss: 0.4230\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4307 - val_loss: 0.4080\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4160 - val_loss: 0.3965\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4030 - val_loss: 0.3861\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3916 - val_loss: 0.3746\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3810 - val_loss: 0.3659\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3718 - val_loss: 0.3564\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3632 - val_loss: 0.3482\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3553 - val_loss: 0.3403\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3478 - val_loss: 0.3331\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3409 - val_loss: 0.3262\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3345 - val_loss: 0.3202\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3287 - val_loss: 0.3150\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3230 - val_loss: 0.3111\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3177 - val_loss: 0.3060\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3127 - val_loss: 0.3009\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3081 - val_loss: 0.2958\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3040 - val_loss: 0.2927\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3000 - val_loss: 0.2876\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2960 - val_loss: 0.2837\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2925 - val_loss: 0.2819\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2890 - val_loss: 0.2783\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2858 - val_loss: 0.2758\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2827 - val_loss: 0.2737\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2798 - val_loss: 0.2695\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2769 - val_loss: 0.2688\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2742 - val_loss: 0.2644\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2717 - val_loss: 0.2619\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2691 - val_loss: 0.2607\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2668 - val_loss: 0.2583\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.2566\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2624 - val_loss: 0.2537\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2601 - val_loss: 0.2507\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2589 - val_loss: 0.2503\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2565 - val_loss: 0.2495\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2546 - val_loss: 0.2460\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2526 - val_loss: 0.2454\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2508 - val_loss: 0.2466\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2495 - val_loss: 0.2417\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2477 - val_loss: 0.2425\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2460 - val_loss: 0.2400\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2447 - val_loss: 0.2396\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2430 - val_loss: 0.2384\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2414 - val_loss: 0.2402\n",
            "Epoch 1/50\n",
            "75/75 [==============================] - 1s 5ms/step - loss: 1.0259 - val_loss: 0.4661\n",
            "Epoch 2/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3993 - val_loss: 0.3017\n",
            "Epoch 3/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.3212 - val_loss: 0.2730\n",
            "Epoch 4/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.2864 - val_loss: 0.2607\n",
            "Epoch 5/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.2576 - val_loss: 0.2255\n",
            "Epoch 6/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.2285 - val_loss: 0.2143\n",
            "Epoch 7/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 0.1955\n",
            "Epoch 8/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.2012 - val_loss: 0.1781\n",
            "Epoch 9/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1890 - val_loss: 0.1708\n",
            "Epoch 10/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1816 - val_loss: 0.2007\n",
            "Epoch 11/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1771 - val_loss: 0.1649\n",
            "Epoch 12/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1704 - val_loss: 0.1761\n",
            "Epoch 13/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1666 - val_loss: 0.1510\n",
            "Epoch 14/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1591 - val_loss: 0.1480\n",
            "Epoch 15/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1544 - val_loss: 0.1494\n",
            "Epoch 16/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1497 - val_loss: 0.1325\n",
            "Epoch 17/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1481 - val_loss: 0.1531\n",
            "Epoch 18/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1451 - val_loss: 0.1406\n",
            "Epoch 19/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1440 - val_loss: 0.1411\n",
            "Epoch 20/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1376 - val_loss: 0.1318\n",
            "Epoch 21/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1352 - val_loss: 0.1482\n",
            "Epoch 22/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1350 - val_loss: 0.1556\n",
            "Epoch 23/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1375 - val_loss: 0.1356\n",
            "Epoch 24/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1293 - val_loss: 0.1381\n",
            "Epoch 25/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 0.1452\n",
            "Epoch 26/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1226 - val_loss: 0.1369\n",
            "Epoch 27/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1188 - val_loss: 0.1369\n",
            "Epoch 28/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1198 - val_loss: 0.1519\n",
            "Epoch 29/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1205 - val_loss: 0.1385\n",
            "Epoch 30/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1115 - val_loss: 0.1612\n",
            "Epoch 31/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1131 - val_loss: 0.1309\n",
            "Epoch 32/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1123 - val_loss: 0.1672\n",
            "Epoch 33/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.1242\n",
            "Epoch 34/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1119 - val_loss: 0.1352\n",
            "Epoch 35/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1085 - val_loss: 0.1329\n",
            "Epoch 36/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 0.1517\n",
            "Epoch 37/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 0.1399\n",
            "Epoch 38/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1001 - val_loss: 0.1658\n",
            "Epoch 39/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1040 - val_loss: 0.1401\n",
            "Epoch 40/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.1087 - val_loss: 0.1271\n",
            "Epoch 41/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 0.1371\n",
            "Epoch 42/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 0.1616\n",
            "Epoch 43/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0916 - val_loss: 0.1410\n",
            "Epoch 44/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 0.1511\n",
            "Epoch 45/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0888 - val_loss: 0.1666\n",
            "Epoch 46/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 0.1633\n",
            "Epoch 47/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 0.2125\n",
            "Epoch 48/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0842 - val_loss: 0.1871\n",
            "Epoch 49/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 0.1667\n",
            "Epoch 50/50\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 0.1662\n",
            "Epoch 1/50\n",
            "50/50 [==============================] - 1s 8ms/step - loss: 1.3429 - val_loss: 0.9832\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.7867 - val_loss: 0.6240\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.5604 - val_loss: 0.4707\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.4601 - val_loss: 0.3967\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.4059 - val_loss: 0.3521\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.3701 - val_loss: 0.3218\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.3439 - val_loss: 0.2996\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.3238 - val_loss: 0.2823\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.3077 - val_loss: 0.2688\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2946 - val_loss: 0.2583\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2837 - val_loss: 0.2495\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2748 - val_loss: 0.2418\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2670 - val_loss: 0.2354\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2604 - val_loss: 0.2301\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2545 - val_loss: 0.2251\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2494 - val_loss: 0.2210\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2448 - val_loss: 0.2172\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2407 - val_loss: 0.2140\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2369 - val_loss: 0.2110\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2335 - val_loss: 0.2080\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2304 - val_loss: 0.2059\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2275 - val_loss: 0.2033\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2249 - val_loss: 0.2015\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2224 - val_loss: 0.1990\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2200 - val_loss: 0.1969\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2179 - val_loss: 0.1957\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2158 - val_loss: 0.1938\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2140 - val_loss: 0.1921\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2122 - val_loss: 0.1906\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2106 - val_loss: 0.1892\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2089 - val_loss: 0.1880\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2074 - val_loss: 0.1866\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2060 - val_loss: 0.1850\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2047 - val_loss: 0.1842\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2036 - val_loss: 0.1834\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2022 - val_loss: 0.1820\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2009 - val_loss: 0.1807\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1997 - val_loss: 0.1801\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1987 - val_loss: 0.1787\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 1s 11ms/step - loss: 0.1974 - val_loss: 0.1780\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.1965 - val_loss: 0.1773\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1957 - val_loss: 0.1759\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1947 - val_loss: 0.1751\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1936 - val_loss: 0.1747\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1927 - val_loss: 0.1734\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1920 - val_loss: 0.1726\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1911 - val_loss: 0.1729\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1902 - val_loss: 0.1716\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1895 - val_loss: 0.1709\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1887 - val_loss: 0.1704\n",
            "Epoch 1/50\n",
            "50/50 [==============================] - 1s 7ms/step - loss: 0.8463 - val_loss: 0.3580\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.3472 - val_loss: 0.2692\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2811 - val_loss: 0.2361\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2504 - val_loss: 0.2199\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2330 - val_loss: 0.2069\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2213 - val_loss: 0.1987\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2127 - val_loss: 0.1928\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2068 - val_loss: 0.1844\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2006 - val_loss: 0.1818\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1960 - val_loss: 0.1768\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1922 - val_loss: 0.1729\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1887 - val_loss: 0.1676\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1851 - val_loss: 0.1665\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1825 - val_loss: 0.1655\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1795 - val_loss: 0.1628\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1773 - val_loss: 0.1621\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1745 - val_loss: 0.1567\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1728 - val_loss: 0.1561\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1701 - val_loss: 0.1522\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1684 - val_loss: 0.1507\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1663 - val_loss: 0.1506\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1647 - val_loss: 0.1498\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1630 - val_loss: 0.1483\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1614 - val_loss: 0.1453\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1599 - val_loss: 0.1446\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1588 - val_loss: 0.1464\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1570 - val_loss: 0.1421\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1561 - val_loss: 0.1421\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1544 - val_loss: 0.1441\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1535 - val_loss: 0.1406\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1526 - val_loss: 0.1421\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1512 - val_loss: 0.1401\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1499 - val_loss: 0.1350\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1493 - val_loss: 0.1382\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1473 - val_loss: 0.1335\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1472 - val_loss: 0.1385\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1456 - val_loss: 0.1394\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1447 - val_loss: 0.1409\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1440 - val_loss: 0.1346\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1425 - val_loss: 0.1341\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1416 - val_loss: 0.1316\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1413 - val_loss: 0.1317\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1400 - val_loss: 0.1304\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1391 - val_loss: 0.1332\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1381 - val_loss: 0.1320\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1378 - val_loss: 0.1313\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1367 - val_loss: 0.1333\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1363 - val_loss: 0.1294\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1352 - val_loss: 0.1327\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1344 - val_loss: 0.1336\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 1.2503 - val_loss: 0.9309\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.7651 - val_loss: 0.5302\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4494 - val_loss: 0.3593\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3684 - val_loss: 0.3069\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3351 - val_loss: 0.2850\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.3121 - val_loss: 0.2577\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2961 - val_loss: 0.2427\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2792 - val_loss: 0.2352\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2693 - val_loss: 0.2293\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2554 - val_loss: 0.2285\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2465 - val_loss: 0.2041\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2315 - val_loss: 0.2143\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2244 - val_loss: 0.2494\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2153 - val_loss: 0.2332\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2094 - val_loss: 0.1836\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2019 - val_loss: 0.1863\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1920 - val_loss: 0.1762\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1913 - val_loss: 0.1700\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1845 - val_loss: 0.1994\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1833 - val_loss: 0.1929\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.1725 - val_loss: 0.2153\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1682 - val_loss: 0.1693\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1647 - val_loss: 0.1696\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1637 - val_loss: 0.1674\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1595 - val_loss: 0.1616\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.1547 - val_loss: 0.1752\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1504 - val_loss: 0.1810\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1496 - val_loss: 0.1639\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1455 - val_loss: 0.1994\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1465 - val_loss: 0.1598\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1412 - val_loss: 0.2288\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1390 - val_loss: 0.1718\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1348 - val_loss: 0.1642\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.1348 - val_loss: 0.1870\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1308 - val_loss: 0.2533\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.1340 - val_loss: 0.1647\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1292 - val_loss: 0.1807\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1221 - val_loss: 0.1758\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 0.1745\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 0.1674\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1189 - val_loss: 0.1916\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1156 - val_loss: 0.2607\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1198 - val_loss: 0.1846\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1143 - val_loss: 0.1694\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1097 - val_loss: 0.1828\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 0.1711\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1040 - val_loss: 0.2251\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1078 - val_loss: 0.1739\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.0990 - val_loss: 0.2167\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1093 - val_loss: 0.1828\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 6s 21ms/step - loss: 1.3711 - val_loss: 1.1563\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 1.0998 - val_loss: 1.0349\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.9509 - val_loss: 0.8200\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.7569 - val_loss: 0.6492\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.6346 - val_loss: 0.5558\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.5654 - val_loss: 0.5112\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.5225 - val_loss: 0.4790\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.4923 - val_loss: 0.4559\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.4655 - val_loss: 0.4438\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.4440 - val_loss: 0.4327\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.4278 - val_loss: 0.4270\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.4146 - val_loss: 0.4401\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.4011 - val_loss: 0.4078\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.3883 - val_loss: 0.3768\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.3804 - val_loss: 0.4085\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.3658 - val_loss: 0.3743\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.3568 - val_loss: 0.3622\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.3509 - val_loss: 0.3460\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.3468 - val_loss: 0.3527\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.3404 - val_loss: 0.6558\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.3383 - val_loss: 0.3328\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.3162 - val_loss: 0.3511\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.3168 - val_loss: 0.3312\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.3046 - val_loss: 0.3357\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.3016 - val_loss: 0.3272\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2911 - val_loss: 0.3344\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2892 - val_loss: 0.3394\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2860 - val_loss: 0.3309\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2870 - val_loss: 0.3315\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2739 - val_loss: 0.3045\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2692 - val_loss: 0.3646\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2777 - val_loss: 0.3772\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2808 - val_loss: 0.3284\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2701 - val_loss: 1.6079\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2956 - val_loss: 0.3179\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2660 - val_loss: 0.3150\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2598 - val_loss: 0.4225\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2607 - val_loss: 0.3848\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2654 - val_loss: 0.3407\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2581 - val_loss: 0.3167\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2441 - val_loss: 0.3813\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2335 - val_loss: 0.3190\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2284 - val_loss: 0.3336\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2332 - val_loss: 0.3216\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2366 - val_loss: 0.3212\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2389 - val_loss: 0.3218\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.2342 - val_loss: 0.3171\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.2346 - val_loss: 0.3015\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 2s 17ms/step - loss: 0.2298 - val_loss: 0.3323\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 3s 17ms/step - loss: 0.2325 - val_loss: 0.3164\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 1.4429 - val_loss: 1.2043\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 1.0711 - val_loss: 0.9431\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.8855 - val_loss: 0.8030\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.7825 - val_loss: 0.7190\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.7187 - val_loss: 0.6644\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.6759 - val_loss: 0.6268\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.6452 - val_loss: 0.5992\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.6217 - val_loss: 0.5779\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.6028 - val_loss: 0.5607\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5870 - val_loss: 0.5463\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5733 - val_loss: 0.5339\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5613 - val_loss: 0.5230\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5505 - val_loss: 0.5133\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5407 - val_loss: 0.5045\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5318 - val_loss: 0.4965\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5235 - val_loss: 0.4892\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5159 - val_loss: 0.4825\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5087 - val_loss: 0.4762\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5021 - val_loss: 0.4703\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4958 - val_loss: 0.4648\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4898 - val_loss: 0.4595\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4842 - val_loss: 0.4545\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4788 - val_loss: 0.4498\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4737 - val_loss: 0.4454\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4688 - val_loss: 0.4411\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4640 - val_loss: 0.4370\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4596 - val_loss: 0.4331\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4553 - val_loss: 0.4294\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 1s 8ms/step - loss: 0.4510 - val_loss: 0.4260\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 0.4470 - val_loss: 0.4224\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.4431 - val_loss: 0.4191\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4393 - val_loss: 0.4158\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4357 - val_loss: 0.4124\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4322 - val_loss: 0.4093\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4289 - val_loss: 0.4063\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4256 - val_loss: 0.4035\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4224 - val_loss: 0.4007\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4194 - val_loss: 0.3980\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4164 - val_loss: 0.3953\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4135 - val_loss: 0.3928\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4107 - val_loss: 0.3901\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4079 - val_loss: 0.3877\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4053 - val_loss: 0.3853\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4027 - val_loss: 0.3830\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4001 - val_loss: 0.3808\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3977 - val_loss: 0.3787\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3953 - val_loss: 0.3765\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3929 - val_loss: 0.3744\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3906 - val_loss: 0.3724\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3884 - val_loss: 0.3703\n",
            "Epoch 1/50\n",
            "50/50 [==============================] - 1s 8ms/step - loss: 0.8036 - val_loss: 0.3471\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.3384 - val_loss: 0.2724\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2822 - val_loss: 0.2411\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2524 - val_loss: 0.2224\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2337 - val_loss: 0.2093\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2215 - val_loss: 0.1996\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.2126 - val_loss: 0.1930\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2062 - val_loss: 0.1875\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.2008 - val_loss: 0.1828\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1959 - val_loss: 0.1802\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1922 - val_loss: 0.1748\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1890 - val_loss: 0.1711\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1862 - val_loss: 0.1712\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1835 - val_loss: 0.1647\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1793 - val_loss: 0.1675\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1773 - val_loss: 0.1661\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1750 - val_loss: 0.1602\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1727 - val_loss: 0.1570\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1713 - val_loss: 0.1527\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1690 - val_loss: 0.1534\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1673 - val_loss: 0.1544\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1656 - val_loss: 0.1521\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1644 - val_loss: 0.1467\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1622 - val_loss: 0.1465\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1607 - val_loss: 0.1510\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1587 - val_loss: 0.1425\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1583 - val_loss: 0.1451\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1563 - val_loss: 0.1430\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1554 - val_loss: 0.1459\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1539 - val_loss: 0.1421\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1531 - val_loss: 0.1388\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1513 - val_loss: 0.1411\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1511 - val_loss: 0.1360\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1496 - val_loss: 0.1380\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1484 - val_loss: 0.1371\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1474 - val_loss: 0.1365\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1467 - val_loss: 0.1380\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1461 - val_loss: 0.1429\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1452 - val_loss: 0.1377\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1442 - val_loss: 0.1357\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1432 - val_loss: 0.1400\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1425 - val_loss: 0.1388\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1406 - val_loss: 0.1368\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1403 - val_loss: 0.1323\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1390 - val_loss: 0.1344\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1382 - val_loss: 0.1304\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 0s 6ms/step - loss: 0.1386 - val_loss: 0.1351\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1374 - val_loss: 0.1321\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1368 - val_loss: 0.1373\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.1357 - val_loss: 0.1307\n",
            "Epoch 1/50\n",
            "50/50 [==============================] - 1s 6ms/step - loss: 1.3968 - val_loss: 1.2075\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 1.0357 - val_loss: 0.9163\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.8317 - val_loss: 0.7580\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.7056 - val_loss: 0.6353\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.6033 - val_loss: 0.5298\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.5169 - val_loss: 0.4450\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.4525 - val_loss: 0.3862\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.4103 - val_loss: 0.3497\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3820 - val_loss: 0.3258\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.3623 - val_loss: 0.3092\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3471 - val_loss: 0.2971\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3348 - val_loss: 0.2868\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.3244 - val_loss: 0.2754\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.3152 - val_loss: 0.2688\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.3069 - val_loss: 0.2622\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2991 - val_loss: 0.2547\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2917 - val_loss: 0.2481\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2851 - val_loss: 0.2437\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2785 - val_loss: 0.2392\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2723 - val_loss: 0.2346\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2665 - val_loss: 0.2318\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2607 - val_loss: 0.2261\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2557 - val_loss: 0.2207\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2509 - val_loss: 0.2174\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2463 - val_loss: 0.2161\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2420 - val_loss: 0.2151\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2384 - val_loss: 0.2094\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2347 - val_loss: 0.2090\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2316 - val_loss: 0.2046\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2286 - val_loss: 0.2015\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2256 - val_loss: 0.1972\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2230 - val_loss: 0.1968\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2205 - val_loss: 0.1977\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2177 - val_loss: 0.1910\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2161 - val_loss: 0.1930\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2131 - val_loss: 0.1879\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2108 - val_loss: 0.1947\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2093 - val_loss: 0.1865\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.2070 - val_loss: 0.1866\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2048 - val_loss: 0.1889\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2032 - val_loss: 0.1846\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.2012 - val_loss: 0.1826\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1990 - val_loss: 0.1791\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1979 - val_loss: 0.1802\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1958 - val_loss: 0.1806\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1943 - val_loss: 0.1780\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1927 - val_loss: 0.1793\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1910 - val_loss: 0.1757\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1895 - val_loss: 0.1770\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 0s 4ms/step - loss: 0.1884 - val_loss: 0.1757\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.6862 - val_loss: 0.2837\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2965 - val_loss: 0.2201\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2273 - val_loss: 0.1809\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1964 - val_loss: 0.1661\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.1818 - val_loss: 0.1632\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1719 - val_loss: 0.1860\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1627 - val_loss: 0.1579\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1581 - val_loss: 0.1527\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1532 - val_loss: 0.1492\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1490 - val_loss: 0.1401\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1468 - val_loss: 0.1669\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1437 - val_loss: 0.1891\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1452 - val_loss: 0.1532\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1386 - val_loss: 0.1518\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1306 - val_loss: 0.1609\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1306 - val_loss: 0.1431\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1201 - val_loss: 0.1556\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1221 - val_loss: 0.1618\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 0.1533\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1193 - val_loss: 0.1446\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1113 - val_loss: 0.1527\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 0.1708\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 0.1587\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1043 - val_loss: 0.1514\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1084 - val_loss: 0.1724\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1029 - val_loss: 0.1624\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 0.1491\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0953 - val_loss: 0.1406\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0927 - val_loss: 0.1491\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0925 - val_loss: 0.1875\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0879 - val_loss: 0.1689\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0885 - val_loss: 0.1813\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0924 - val_loss: 0.1878\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.1642\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0857 - val_loss: 0.1808\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0885 - val_loss: 0.1697\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0794 - val_loss: 0.1810\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 0.2118\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.1733\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0724 - val_loss: 0.1702\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.1826\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 0.2500\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0800 - val_loss: 0.2106\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.2290\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.2286\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0554 - val_loss: 0.2215\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.2111\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.2276\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.2295\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0568 - val_loss: 0.2424\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - 2s 5ms/step - loss: 0.8834 - val_loss: 0.5170\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4964 - val_loss: 0.4066\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4169 - val_loss: 0.3569\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3632 - val_loss: 0.3220\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3343 - val_loss: 0.3424\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3125 - val_loss: 0.3169\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2965 - val_loss: 0.2914\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2875 - val_loss: 0.3358\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2774 - val_loss: 0.2809\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2698 - val_loss: 0.2646\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2702 - val_loss: 0.2709\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2526 - val_loss: 0.2503\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2582 - val_loss: 0.3046\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2537 - val_loss: 0.2700\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2428 - val_loss: 0.2443\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2402 - val_loss: 0.2538\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2535 - val_loss: 0.3057\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2330 - val_loss: 0.2480\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2255 - val_loss: 0.2306\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2251 - val_loss: 0.2472\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2208 - val_loss: 0.2361\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2162 - val_loss: 0.2396\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 0.2273\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2181 - val_loss: 0.2430\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2235 - val_loss: 0.2464\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2106 - val_loss: 0.2507\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1997 - val_loss: 0.2743\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2089 - val_loss: 0.2541\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2011 - val_loss: 0.2367\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.2020 - val_loss: 0.2380\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1924 - val_loss: 0.2467\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1984 - val_loss: 0.2345\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1936 - val_loss: 0.2402\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1891 - val_loss: 0.2614\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1968 - val_loss: 0.2718\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1929 - val_loss: 0.2869\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 0.2578\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1873 - val_loss: 0.2364\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1845 - val_loss: 0.2488\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.1875 - val_loss: 0.2321\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1855 - val_loss: 0.2433\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1818 - val_loss: 0.2709\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1870 - val_loss: 0.2333\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1719 - val_loss: 0.2563\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1692 - val_loss: 0.2309\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 0.2719\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1656 - val_loss: 0.2238\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.1766 - val_loss: 0.2466\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1694 - val_loss: 0.2536\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1680 - val_loss: 0.2733\n",
            "Epoch 1/50\n",
            "75/75 [==============================] - 2s 10ms/step - loss: 1.3268 - val_loss: 1.0875\n",
            "Epoch 2/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 1.0613 - val_loss: 1.0283\n",
            "Epoch 3/50\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.9780 - val_loss: 0.8250\n",
            "Epoch 4/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.6598 - val_loss: 0.5174\n",
            "Epoch 5/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.5142 - val_loss: 0.4604\n",
            "Epoch 6/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4679 - val_loss: 0.4193\n",
            "Epoch 7/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4301 - val_loss: 0.4104\n",
            "Epoch 8/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.4001 - val_loss: 0.3786\n",
            "Epoch 9/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3991 - val_loss: 0.3726\n",
            "Epoch 10/50\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.3659 - val_loss: 0.3425\n",
            "Epoch 11/50\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.3440 - val_loss: 0.3501\n",
            "Epoch 12/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3720 - val_loss: 0.3633\n",
            "Epoch 13/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3387 - val_loss: 0.3206\n",
            "Epoch 14/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3134 - val_loss: 0.3142\n",
            "Epoch 15/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3025 - val_loss: 0.3341\n",
            "Epoch 16/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2924 - val_loss: 0.3082\n",
            "Epoch 17/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2826 - val_loss: 0.2849\n",
            "Epoch 18/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2822 - val_loss: 0.3168\n",
            "Epoch 19/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2822 - val_loss: 0.3086\n",
            "Epoch 20/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2747 - val_loss: 0.3058\n",
            "Epoch 21/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2619 - val_loss: 0.3497\n",
            "Epoch 22/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2525 - val_loss: 0.2862\n",
            "Epoch 23/50\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.2677 - val_loss: 0.2839\n",
            "Epoch 24/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2506 - val_loss: 0.3310\n",
            "Epoch 25/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2473 - val_loss: 0.3034\n",
            "Epoch 26/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2541 - val_loss: 0.3224\n",
            "Epoch 27/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2618 - val_loss: 0.3841\n",
            "Epoch 28/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2399 - val_loss: 0.3033\n",
            "Epoch 29/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2275 - val_loss: 0.2872\n",
            "Epoch 30/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2453 - val_loss: 0.3016\n",
            "Epoch 31/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2316 - val_loss: 0.3485\n",
            "Epoch 32/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2334 - val_loss: 0.2829\n",
            "Epoch 33/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2275 - val_loss: 0.2596\n",
            "Epoch 34/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2382 - val_loss: 0.2828\n",
            "Epoch 35/50\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2366 - val_loss: 0.3143\n",
            "Epoch 36/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2134 - val_loss: 0.2658\n",
            "Epoch 37/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2088 - val_loss: 0.3233\n",
            "Epoch 38/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2162 - val_loss: 0.2936\n",
            "Epoch 39/50\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2105 - val_loss: 0.2860\n",
            "Epoch 40/50\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2139 - val_loss: 0.3573\n",
            "Epoch 41/50\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.1947 - val_loss: 0.2695\n",
            "Epoch 42/50\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.2060 - val_loss: 0.2766\n",
            "Epoch 43/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2040 - val_loss: 0.2764\n",
            "Epoch 44/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2242 - val_loss: 0.2906\n",
            "Epoch 45/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1973 - val_loss: 0.2961\n",
            "Epoch 46/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1936 - val_loss: 0.2759\n",
            "Epoch 47/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1771 - val_loss: 0.3363\n",
            "Epoch 48/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.2038 - val_loss: 0.2717\n",
            "Epoch 49/50\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.1866 - val_loss: 0.3567\n",
            "Epoch 50/50\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.1875 - val_loss: 0.2990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "performance_df"
      ],
      "metadata": {
        "id": "yf4oipftE1Nr",
        "outputId": "4bb3079b-dfc6-4ebf-a534-8ad1de00411c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   test_accuracy n_neurons n_layers dropout learning_rate momentum batch_size  \\\n",
              "0       0.946956        64        5     0.1         0.005      0.9         60   \n",
              "1        0.94997        64        2     0.1         0.001      0.9         60   \n",
              "2       0.952984       256        2     0.1         0.001      0.9         20   \n",
              "3       0.918626        64       10     0.2         0.001      0.5         20   \n",
              "4       0.952381        64        2     0.2         0.001      0.9         60   \n",
              "5       0.952984        64        5     0.1         0.005      0.5         40   \n",
              "6         0.9566       128        5     0.2         0.005      0.5         20   \n",
              "7       0.952381       256       10     0.2         0.001      0.9         60   \n",
              "8       0.953586        64        2     0.4         0.005      0.5         20   \n",
              "9       0.951175        64        5     0.1         0.005      0.9         40   \n",
              "10      0.952381       256        2     0.2         0.001      0.9         60   \n",
              "11      0.954792       256        2     0.4         0.005      0.9         60   \n",
              "12       0.94575        64       10     0.4         0.005      0.5         20   \n",
              "13      0.931284       256       10     0.1         0.005      0.5         20   \n",
              "14      0.949367       256        2     0.4         0.001      0.5         20   \n",
              "15      0.955998       256        2     0.2         0.005      0.9         60   \n",
              "16      0.951175        64        5     0.4         0.005      0.5         60   \n",
              "17      0.942737        64        5     0.1         0.005      0.9         20   \n",
              "18      0.953586        64        5     0.4         0.005      0.9         20   \n",
              "19      0.952381       128       10     0.4         0.005      0.9         40   \n",
              "\n",
              "   batch_norm l2_reg leakiness_ReLU  \n",
              "0       False   True            0.2  \n",
              "1       False  False            0.1  \n",
              "2       False   True            0.2  \n",
              "3        True  False            0.1  \n",
              "4       False  False            0.2  \n",
              "5       False   True            0.2  \n",
              "6        True  False            0.2  \n",
              "7        True  False            0.1  \n",
              "8       False   True            0.1  \n",
              "9       False  False            0.1  \n",
              "10       True  False            0.1  \n",
              "11       True  False            0.2  \n",
              "12       True  False            0.1  \n",
              "13      False   True            0.1  \n",
              "14       True   True            0.1  \n",
              "15       True  False            0.2  \n",
              "16       True  False            0.1  \n",
              "17       True  False            0.1  \n",
              "18       True   True            0.2  \n",
              "19      False   True            0.1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-14dc316c-df6d-4de7-b40e-2e1dffc134df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>n_neurons</th>\n",
              "      <th>n_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>momentum</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>batch_norm</th>\n",
              "      <th>l2_reg</th>\n",
              "      <th>leakiness_ReLU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.946956</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>60</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.94997</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>60</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.952984</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.918626</td>\n",
              "      <td>64</td>\n",
              "      <td>10</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.5</td>\n",
              "      <td>20</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.952381</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>60</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.952984</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>40</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.9566</td>\n",
              "      <td>128</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>20</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.952381</td>\n",
              "      <td>256</td>\n",
              "      <td>10</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>60</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.953586</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>20</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.951175</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>40</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.952381</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>60</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.954792</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>60</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.94575</td>\n",
              "      <td>64</td>\n",
              "      <td>10</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>20</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.931284</td>\n",
              "      <td>256</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>20</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.949367</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.5</td>\n",
              "      <td>20</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.955998</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>60</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.951175</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.5</td>\n",
              "      <td>60</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.942737</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.953586</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.952381</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>40</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14dc316c-df6d-4de7-b40e-2e1dffc134df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-14dc316c-df6d-4de7-b40e-2e1dffc134df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-14dc316c-df6d-4de7-b40e-2e1dffc134df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "performance_df['test_accuracy'].astype(float)"
      ],
      "metadata": {
        "id": "EcGhzG-SGGLY",
        "outputId": "0ba2154e-fce7-496b-f259-41d80e84ec3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0.946956\n",
              "1     0.949970\n",
              "2     0.952984\n",
              "3     0.918626\n",
              "4     0.952381\n",
              "5     0.952984\n",
              "6     0.956600\n",
              "7     0.952381\n",
              "8     0.953586\n",
              "9     0.951175\n",
              "10    0.952381\n",
              "11    0.954792\n",
              "12    0.945750\n",
              "13    0.931284\n",
              "14    0.949367\n",
              "15    0.955998\n",
              "16    0.951175\n",
              "17    0.942737\n",
              "18    0.953586\n",
              "19    0.952381\n",
              "Name: test_accuracy, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "performance_df['test_accuracy'].astype(float).corr(performance_df['n_neurons'].astype(float))"
      ],
      "metadata": {
        "id": "URnrnzg5CkUq",
        "outputId": "b36c7b5e-03ad-43c2-c8a4-caf59e94e178",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13406756821673466"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scvi\n",
        "\n",
        "scvi.model.SCVI.setup_anndata(adata, batch_key=\"batch\")\n",
        "vae = scvi.model.SCVI(adata)\n",
        "vae.train(max_epochs=100)\n",
        "adata.obsm['X_scvi'] = vae.get_latent_representation()"
      ],
      "metadata": {
        "id": "bOP93pMra7YO",
        "outputId": "43b9d4c9-74f3-4c2e-c3ab-9fbe78c406cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 0\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/100: 100%|██████████| 100/100 [08:59<00:00,  5.40s/it, loss=764, v_num=1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adata.obsm['X_scVI'] = vae.get_latent_representation()\n",
        "del adata.obsm['X_scvi']"
      ],
      "metadata": {
        "id": "yDJlgX4aegb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata.obs['Level_3'].replace('ILC', 'ILC_ALL')"
      ],
      "metadata": {
        "id": "c3PZXuvOe_hs",
        "outputId": "20d18181-9628-427b-9365-e559e5e4fb02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CATTATCCACAGCGTC.Post_P013_b          T\n",
              "ATCATGGCAAGGTGTG.Post_P023_b          T\n",
              "CGTGTAAAGACAGACC.Pre_P013_b           T\n",
              "TTGGAACCATCGATTG.Pre_P014_b     ILC_ALL\n",
              "AGCTCTCTCTCACATT.Post_P018_b          T\n",
              "                                 ...   \n",
              "AGAGTGGGTACGACCC.Prog_P013_b    ILC_ALL\n",
              "CAAGATCTCAACGGCC.Pre_P008_b           T\n",
              "GAGGTGAAGGCCCTCA.Pre_P014_b           T\n",
              "CCATGTCCAGCCACCA.Post_P017_b          T\n",
              "ATCGAGTTCATACGGT.Post_P002_b          T\n",
              "Name: Level_3, Length: 5000, dtype: category\n",
              "Categories (2, object): ['ILC_ALL', 'T']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adata.write('drive/MyDrive/TNK_Zhang_BRCA_labeled_Test.h5ad')"
      ],
      "metadata": {
        "id": "iRMA0AIbcrJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adata = sc.read('drive/MyDrive/single_cell/processed/pbmc3k_level_1_scvi.h5ad')"
      ],
      "metadata": {
        "id": "MIoF8UQJRf9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adata.obs['leiden']"
      ],
      "metadata": {
        "id": "rU0gryAt9Buo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adata.obs['training_level_1']"
      ],
      "metadata": {
        "id": "EOteQm1jXdLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#neues erstellen des digraphs, for loop setzt alle node memories zurück\n",
        "#hc = Hierarchical_Classifier({'all': {'TNK':{'CD4 T':{}, 'CD8 T':{}, 'NK':{}}, 'Myeloisch':{'FCGR3A Monocytes': {}, 'Dendritic':{}, 'CD14 Monocytes':{}}, 'B': {}, 'Others':{'Megakaryocytes':{}}}})\n",
        "hc = Hierarchical_Classifier(\n",
        "  {\n",
        "    'T_ILC': {\n",
        "      'T': {\n",
        "        'ab_T': {}, \n",
        "        'MAIT': {}, \n",
        "        'gd_T': {}\n",
        "      }, \n",
        "      'ILC_ALL': {\n",
        "        'NK': {},\n",
        "        'ILC': {}\n",
        "      } \n",
        "    }\n",
        "  }\n",
        ")\n",
        "hc.make_classifier_graph()\n",
        "for node in hc.graph.nodes:\n",
        "  hc.init_node_memory_object(node)\n",
        "nx.draw(hc.graph, with_labels=True)"
      ],
      "metadata": {
        "id": "AopRfZquzw_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "d4731c82-4f28-4e95-f75a-882c7b62b2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list_subgraph_nodes(hc.graph, 'Myeloisch')"
      ],
      "metadata": {
        "id": "otQ0mLmh82GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hc.group_labels_of_subgraph_to_parent_label('all')"
      ],
      "metadata": {
        "id": "5-bVX9M4afKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata = adata[adata.obs['Level_4'] != '']"
      ],
      "metadata": {
        "id": "ov905XqLhNun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save input data and initialize first network in first node\n",
        "hc.init_node_memory_object('T_ILC', [adata.obsm['X_scVI'], adata.obs['Level_4'], Neural_Network])"
      ],
      "metadata": {
        "id": "mP-BYisi-cda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ebc7fd-0d91-4cd9-f86d-148cba27a6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aus init_node_memory_object von Knoten T_ILC: output größe initialisiert als 2\n",
            "zu übergebene y_input daten: ['T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#all set up - run local classifier\n",
        "hc.run_local_classifier('T_ILC')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZlrPIwMDbeb",
        "outputId": "443f6cbf-2551-4073-f5c4-db4e291c1d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "network y_input before encoding: ['T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'T', 'T', 'T', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'ILC_ALL', 'ILC_ALL', 'T', 'T', 'T', 'T']\n",
            "Epoch 1/50\n",
            "47/47 [==============================] - 1s 5ms/step - loss: 1.0096 - val_loss: 0.7590\n",
            "Epoch 2/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.6042 - val_loss: 0.5265\n",
            "Epoch 3/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4466 - val_loss: 0.4131\n",
            "Epoch 4/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.3618 - val_loss: 0.3417\n",
            "Epoch 5/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.3067 - val_loss: 0.2918\n",
            "Epoch 6/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.2678 - val_loss: 0.2557\n",
            "Epoch 7/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.2389 - val_loss: 0.2288\n",
            "Epoch 8/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.2168 - val_loss: 0.2080\n",
            "Epoch 9/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1996 - val_loss: 0.1913\n",
            "Epoch 10/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1857 - val_loss: 0.1778\n",
            "Epoch 11/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1743 - val_loss: 0.1667\n",
            "Epoch 12/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 0.1575\n",
            "Epoch 13/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1571 - val_loss: 0.1498\n",
            "Epoch 14/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1504 - val_loss: 0.1433\n",
            "Epoch 15/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1447 - val_loss: 0.1378\n",
            "Epoch 16/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1400 - val_loss: 0.1328\n",
            "Epoch 17/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1357 - val_loss: 0.1288\n",
            "Epoch 18/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1320 - val_loss: 0.1251\n",
            "Epoch 19/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 0.1218\n",
            "Epoch 20/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1259 - val_loss: 0.1189\n",
            "Epoch 21/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1233 - val_loss: 0.1164\n",
            "Epoch 22/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1210 - val_loss: 0.1142\n",
            "Epoch 23/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1189 - val_loss: 0.1122\n",
            "Epoch 24/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1171 - val_loss: 0.1105\n",
            "Epoch 25/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 0.1087\n",
            "Epoch 26/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1139 - val_loss: 0.1073\n",
            "Epoch 27/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 0.1059\n",
            "Epoch 28/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1112 - val_loss: 0.1046\n",
            "Epoch 29/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1101 - val_loss: 0.1036\n",
            "Epoch 30/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1090 - val_loss: 0.1025\n",
            "Epoch 31/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1080 - val_loss: 0.1015\n",
            "Epoch 32/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1071 - val_loss: 0.1006\n",
            "Epoch 33/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1062 - val_loss: 0.0998\n",
            "Epoch 34/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1054 - val_loss: 0.0990\n",
            "Epoch 35/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1047 - val_loss: 0.0982\n",
            "Epoch 36/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1039 - val_loss: 0.0976\n",
            "Epoch 37/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1033 - val_loss: 0.0969\n",
            "Epoch 38/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1026 - val_loss: 0.0962\n",
            "Epoch 39/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1020 - val_loss: 0.0958\n",
            "Epoch 40/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.0952\n",
            "Epoch 41/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1009 - val_loss: 0.0947\n",
            "Epoch 42/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1004 - val_loss: 0.0942\n",
            "Epoch 43/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.0999 - val_loss: 0.0938\n",
            "Epoch 44/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.0994 - val_loss: 0.0933\n",
            "Epoch 45/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.0990 - val_loss: 0.0929\n",
            "Epoch 46/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.0985 - val_loss: 0.0925\n",
            "Epoch 47/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.0981 - val_loss: 0.0921\n",
            "Epoch 48/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0917\n",
            "Epoch 49/50\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.0973 - val_loss: 0.0914\n",
            "Epoch 50/50\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.0969 - val_loss: 0.0910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print first level classifier model accuracy\n",
        "print(hc.graph.nodes['T_ILC']['memory'].training_acc)\n",
        "print(hc.graph.nodes['T_ILC']['memory'].test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q5Yy5KrNtTp",
        "outputId": "89300b29-3dab-4f2b-9388-b1107e009c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9731604342581424\n",
            "0.9770946353224834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hc.graph.nodes['T_ILC']['memory'].local_classifier.label_encoder.inverse_transform(hc.graph.nodes['T_ILC']['memory'].training_prediction_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjVQ3DdXHD7q",
        "outputId": "aea9291a-04ee-4c1b-9466-540f484004e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['T', 'T', 'T', ..., 'T', 'T', 'T'], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#seperate output of first local classifier to local classifiers of child nodes\n",
        "hc.subset_pred_vec('T_ILC')"
      ],
      "metadata": {
        "id": "jtDy-HIaCB-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hc.graph.nodes['TNK']['memory'].__dict__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsMW-iVFBr5G",
        "outputId": "4637b4a5-4d85-4201-dea4-24b0d7feed4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apriori_y_input_data_indices': (array([], dtype=int64),),\n",
              " 'classifier_class': None,\n",
              " 'local_classifier_params': [],\n",
              " 'test_acc': None,\n",
              " 'test_prediction_vec': None,\n",
              " 'training_acc': None,\n",
              " 'training_prediction_vec': None,\n",
              " 'x_input_data': array([[-0.8241001 , -0.44704255, -0.4074863 , ...,  1.5713328 ,\n",
              "         -1.546495  , -1.4017861 ],\n",
              "        [-1.9270294 , -0.00890738,  0.97932434, ..., -0.6752965 ,\n",
              "          0.28452635, -0.03110772],\n",
              "        [-0.67081773, -0.9181075 , -0.8583566 , ...,  2.500195  ,\n",
              "         -0.4206484 ,  1.1639605 ],\n",
              "        ...,\n",
              "        [-1.5681272 ,  1.3090951 ,  0.6303377 , ..., -1.2559723 ,\n",
              "          0.4473528 ,  0.57438225],\n",
              "        [ 0.01072386,  1.6088051 ,  1.7508733 , ..., -0.53124595,\n",
              "          1.1915506 , -1.2604463 ],\n",
              "        [-0.73548514, -0.02796707, -0.24904236, ...,  0.5823264 ,\n",
              "         -0.28247118, -0.26726872]], dtype=float32),\n",
              " 'y_input_data': array(['CD8 T', 'CD4 T', 'CD4 T', ..., 'CD4 T', 'B', 'CD4 T'],\n",
              "       dtype=object),\n",
              " 'y_input_grouped_labels': None}"
            ]
          },
          "metadata": {},
          "execution_count": 476
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zeigt input vektor bevor falsche zellen herausgeworfen wurden --> falsch zugeordnete Zellen zu sehen (hier Monozyten)\n",
        "hc.graph.nodes['TNK']['memory'].y_input_data[0:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSgxang3TOnx",
        "outputId": "e20825e0-e094-4e89-d951-8fa1c3ec0a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CD8 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD8 T', 'CD8 T', 'CD4 T',\n",
              "       'CD4 T', 'CD8 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T',\n",
              "       'NK', 'CD4 T', 'CD4 T', 'NK', 'CD8 T', 'CD4 T', 'NK', 'CD4 T',\n",
              "       'CD8 T', 'CD8 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T',\n",
              "       'CD4 T', 'NK', 'NK', 'NK', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T',\n",
              "       'CD4 T', 'NK', 'CD4 T', 'NK', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T',\n",
              "       'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD8 T',\n",
              "       'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD8 T',\n",
              "       'CD8 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'NK',\n",
              "       'CD4 T', 'CD8 T', 'CD8 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T',\n",
              "       'CD4 T', 'CD4 T', 'CD8 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T',\n",
              "       'CD8 T', 'CD8 T', 'CD4 T', 'CD8 T', 'CD4 T', 'CD4 T', 'CD4 T',\n",
              "       'CD4 T', 'CD4 T', 'CD8 T', 'CD4 T', 'CD4 T', 'CD8 T', 'CD4 T',\n",
              "       'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T', 'CD4 T'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 479
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nun einmal alles für den tnk node \n",
        "#NN classifier initialisieren, hier noch unnötige Übergabe von input Daten, die er schon hat -> anders lösen, anderen Default, oder anderes initialisieren\n",
        "hc.init_node_memory_object('T', [hc.graph.nodes['T']['memory'].x_input_data, hc.graph.nodes['T']['memory'].y_input_data, Neural_Network])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oopqt6s5PnBt",
        "outputId": "bf4998ad-0f85-4b3d-8774-14e60c410cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aus init_node_memory_object von Knoten T: output größe initialisiert als 3\n",
            "zu übergebene y_input daten: ['ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hc.run_local_classifier('T')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm4AdFeBQTB2",
        "outputId": "1ff2b1c4-d302-4484-f6af-2544e7450abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "network y_input before encoding: ['ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'gd_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'gd_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'gd_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'MAIT', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T', 'ab_T']\n",
            "Epoch 1/50\n",
            "33/33 [==============================] - 1s 11ms/step - loss: 0.7295 - val_loss: 0.6378\n",
            "Epoch 2/50\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.5504 - val_loss: 0.4947\n",
            "Epoch 3/50\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.4546 - val_loss: 0.4251\n",
            "Epoch 4/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.4041 - val_loss: 0.3846\n",
            "Epoch 5/50\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3729 - val_loss: 0.3570\n",
            "Epoch 6/50\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.3505 - val_loss: 0.3370\n",
            "Epoch 7/50\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.3334 - val_loss: 0.3211\n",
            "Epoch 8/50\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.3193 - val_loss: 0.3080\n",
            "Epoch 9/50\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.3072 - val_loss: 0.2971\n",
            "Epoch 10/50\n",
            "33/33 [==============================] - 0s 13ms/step - loss: 0.2968 - val_loss: 0.2874\n",
            "Epoch 11/50\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 0.2874 - val_loss: 0.2790\n",
            "Epoch 12/50\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.2790 - val_loss: 0.2713\n",
            "Epoch 13/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2713 - val_loss: 0.2644\n",
            "Epoch 14/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2641 - val_loss: 0.2582\n",
            "Epoch 15/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2578 - val_loss: 0.2522\n",
            "Epoch 16/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2516 - val_loss: 0.2469\n",
            "Epoch 17/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2459 - val_loss: 0.2417\n",
            "Epoch 18/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2406 - val_loss: 0.2370\n",
            "Epoch 19/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2356 - val_loss: 0.2327\n",
            "Epoch 20/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2310 - val_loss: 0.2286\n",
            "Epoch 21/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2266 - val_loss: 0.2247\n",
            "Epoch 22/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2224 - val_loss: 0.2211\n",
            "Epoch 23/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2185 - val_loss: 0.2176\n",
            "Epoch 24/50\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.2147 - val_loss: 0.2144\n",
            "Epoch 25/50\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.2112 - val_loss: 0.2114\n",
            "Epoch 26/50\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.2079 - val_loss: 0.2083\n",
            "Epoch 27/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.2047 - val_loss: 0.2055\n",
            "Epoch 28/50\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 0.2017 - val_loss: 0.2029\n",
            "Epoch 29/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1988 - val_loss: 0.2004\n",
            "Epoch 30/50\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.1961 - val_loss: 0.1979\n",
            "Epoch 31/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1935 - val_loss: 0.1957\n",
            "Epoch 32/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1910 - val_loss: 0.1933\n",
            "Epoch 33/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1886 - val_loss: 0.1912\n",
            "Epoch 34/50\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.1863 - val_loss: 0.1893\n",
            "Epoch 35/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1841 - val_loss: 0.1873\n",
            "Epoch 36/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1821 - val_loss: 0.1854\n",
            "Epoch 37/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1801 - val_loss: 0.1836\n",
            "Epoch 38/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1782 - val_loss: 0.1818\n",
            "Epoch 39/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1763 - val_loss: 0.1802\n",
            "Epoch 40/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1745 - val_loss: 0.1785\n",
            "Epoch 41/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1728 - val_loss: 0.1771\n",
            "Epoch 42/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1712 - val_loss: 0.1755\n",
            "Epoch 43/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1696 - val_loss: 0.1741\n",
            "Epoch 44/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1681 - val_loss: 0.1726\n",
            "Epoch 45/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1666 - val_loss: 0.1713\n",
            "Epoch 46/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1652 - val_loss: 0.1700\n",
            "Epoch 47/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1638 - val_loss: 0.1687\n",
            "Epoch 48/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1625 - val_loss: 0.1674\n",
            "Epoch 49/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1612 - val_loss: 0.1664\n",
            "Epoch 50/50\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 0.1652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy of tnk classifier model \n",
        "print(hc.graph.nodes['T']['memory'].training_acc)\n",
        "\n",
        "print(hc.graph.nodes['T']['memory'].test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDK0Axx1iq_X",
        "outputId": "0a4f6d02-81b6-4450-af6e-46d3d0f1bdc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9499568593615185\n",
            "0.9646246764452114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show saved prediction results in next labels, <- hm ne das wird erst später aufgerufen\n",
        "hc.graph.nodes['ILC']['memory'].__dict__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqMhrlqMi45G",
        "outputId": "7bae7fc5-3a18-4e7d-8877-047509481908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apriori_y_input_data_indices': None,\n",
              " 'classifier_class': __main__.Neural_Network,\n",
              " 'local_classifier_params': [],\n",
              " 'test_acc': None,\n",
              " 'test_prediction_vec': None,\n",
              " 'training_acc': None,\n",
              " 'training_prediction_vec': None,\n",
              " 'x_input_data': None,\n",
              " 'y_input_data': None,\n",
              " 'y_input_grouped_labels': None}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show status of node Myeloische Zellen\n",
        "hc.graph.nodes['Myeloisch']['memory'].__dict__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmjgeNI0wdfE",
        "outputId": "c3994b11-4e8a-4a9f-bc81-d3dc41470363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apriori_y_input_data_indices': (array([], dtype=int64),),\n",
              " 'classifier_class': None,\n",
              " 'local_classifier_params': [],\n",
              " 'test_acc': None,\n",
              " 'test_prediction_vec': None,\n",
              " 'training_acc': None,\n",
              " 'training_prediction_vec': None,\n",
              " 'x_input_data': array([[-0.65018964,  0.5909275 ,  0.17820597, ...,  0.7609098 ,\n",
              "         -0.9427402 ,  1.1240003 ],\n",
              "        [ 1.3202016 ,  0.6382091 , -1.6076678 , ..., -1.3887111 ,\n",
              "         -0.43054047, -1.2502768 ],\n",
              "        [ 0.03016484, -0.13684195,  0.38049364, ..., -0.8243723 ,\n",
              "         -0.4725106 ,  0.7852393 ],\n",
              "        ...,\n",
              "        [ 0.00502422,  0.18570839,  0.38839135, ..., -0.05192301,\n",
              "         -1.1637948 ,  1.4046977 ],\n",
              "        [ 1.5008779 , -0.13873322, -0.09292659, ...,  0.1126502 ,\n",
              "         -1.7490447 ,  1.5059206 ],\n",
              "        [ 0.57691723, -0.09258436,  0.77182037, ..., -0.11639237,\n",
              "          0.35169557,  1.8422029 ]], dtype=float32),\n",
              " 'y_input_data': array(['NK', 'FCGR3A Monocytes', 'NK', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD8 T', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'FCGR3A Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'Dendritic',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'Dendritic', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes', 'NK',\n",
              "        'CD8 T', 'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD8 T', 'CD14 Monocytes', 'Dendritic',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'Dendritic', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'NK', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'Dendritic', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'FCGR3A Monocytes',\n",
              "        'Dendritic', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'Dendritic',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes', 'NK', 'NK',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'NK', 'NK',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'Dendritic',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'NK',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'NK', 'FCGR3A Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'Dendritic',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'Dendritic', 'NK', 'NK',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'NK',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'Dendritic', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD8 T', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'Dendritic', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'CD4 T',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'NK', 'Dendritic', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'FCGR3A Monocytes', 'Dendritic', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'FCGR3A Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'Dendritic',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'NK', 'Dendritic',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'Megakaryocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'Dendritic', 'NK', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'Dendritic', 'FCGR3A Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD8 T', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'NK', 'CD14 Monocytes', 'CD4 T',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'Dendritic', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'Dendritic', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD8 T', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD4 T', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD8 T', 'Megakaryocytes', 'NK',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'Dendritic',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'NK', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD8 T', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD8 T', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD8 T', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'Dendritic', 'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'NK', 'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'NK',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'NK', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'NK', 'CD14 Monocytes',\n",
              "        'CD8 T', 'NK', 'CD14 Monocytes', 'NK', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'FCGR3A Monocytes', 'Dendritic', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'Dendritic', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'NK', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'NK', 'Dendritic', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes', 'Dendritic',\n",
              "        'CD14 Monocytes', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'FCGR3A Monocytes', 'CD14 Monocytes', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'NK', 'FCGR3A Monocytes',\n",
              "        'CD14 Monocytes', 'Dendritic', 'CD14 Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'FCGR3A Monocytes', 'CD14 Monocytes',\n",
              "        'CD14 Monocytes', 'NK', 'CD14 Monocytes', 'CD14 Monocytes'],\n",
              "       dtype=object),\n",
              " 'y_input_grouped_labels': None}"
            ]
          },
          "metadata": {},
          "execution_count": 484
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initialisieren des local classifiers für den Myeloisch node\n",
        "hc.init_node_memory_object('ILC_ALL', memory_class_params=[hc.graph.nodes['ILC_ALL']['memory'].x_input_data, hc.graph.nodes['ILC_ALL']['memory'].y_input_data, Neural_Network])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMEh54iaww_F",
        "outputId": "817e98d1-932c-4078-ae0d-f4881629e4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aus init_node_memory_object von Knoten ILC_ALL: output größe initialisiert als 2\n",
            "zu übergebene y_input daten: ['NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hc.run_local_classifier('ILC_ALL')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfjzJDFLwrZx",
        "outputId": "a0315b9a-f870-4687-d45b-502911fcc197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "network y_input before encoding: ['NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'ILC', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK', 'NK']\n",
            "Epoch 1/50\n",
            "13/13 [==============================] - 1s 25ms/step - loss: 0.4963 - val_loss: 0.4571\n",
            "Epoch 2/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4275 - val_loss: 0.3933\n",
            "Epoch 3/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.3582 - val_loss: 0.3420\n",
            "Epoch 4/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.3049 - val_loss: 0.3052\n",
            "Epoch 5/50\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2651 - val_loss: 0.2790\n",
            "Epoch 6/50\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2360 - val_loss: 0.2598\n",
            "Epoch 7/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.2139 - val_loss: 0.2452\n",
            "Epoch 8/50\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1966 - val_loss: 0.2339\n",
            "Epoch 9/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1823 - val_loss: 0.2252\n",
            "Epoch 10/50\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1713 - val_loss: 0.2179\n",
            "Epoch 11/50\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1618 - val_loss: 0.2121\n",
            "Epoch 12/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1535 - val_loss: 0.2073\n",
            "Epoch 13/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1468 - val_loss: 0.2032\n",
            "Epoch 14/50\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1409 - val_loss: 0.1997\n",
            "Epoch 15/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1358 - val_loss: 0.1966\n",
            "Epoch 16/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1313 - val_loss: 0.1941\n",
            "Epoch 17/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1272 - val_loss: 0.1917\n",
            "Epoch 18/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1237 - val_loss: 0.1898\n",
            "Epoch 19/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1205 - val_loss: 0.1880\n",
            "Epoch 20/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1177 - val_loss: 0.1865\n",
            "Epoch 21/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1150 - val_loss: 0.1850\n",
            "Epoch 22/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1127 - val_loss: 0.1837\n",
            "Epoch 23/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1105 - val_loss: 0.1826\n",
            "Epoch 24/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1085 - val_loss: 0.1814\n",
            "Epoch 25/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1067 - val_loss: 0.1805\n",
            "Epoch 26/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1050 - val_loss: 0.1794\n",
            "Epoch 27/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1035 - val_loss: 0.1786\n",
            "Epoch 28/50\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1020 - val_loss: 0.1778\n",
            "Epoch 29/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1007 - val_loss: 0.1770\n",
            "Epoch 30/50\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0994 - val_loss: 0.1762\n",
            "Epoch 31/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0983 - val_loss: 0.1756\n",
            "Epoch 32/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0972 - val_loss: 0.1749\n",
            "Epoch 33/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0961 - val_loss: 0.1742\n",
            "Epoch 34/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0951 - val_loss: 0.1736\n",
            "Epoch 35/50\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0943 - val_loss: 0.1730\n",
            "Epoch 36/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0933 - val_loss: 0.1724\n",
            "Epoch 37/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0925 - val_loss: 0.1718\n",
            "Epoch 38/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0917 - val_loss: 0.1713\n",
            "Epoch 39/50\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.0909 - val_loss: 0.1707\n",
            "Epoch 40/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0902 - val_loss: 0.1701\n",
            "Epoch 41/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0895 - val_loss: 0.1695\n",
            "Epoch 42/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0889 - val_loss: 0.1690\n",
            "Epoch 43/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0882 - val_loss: 0.1684\n",
            "Epoch 44/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0876 - val_loss: 0.1679\n",
            "Epoch 45/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0870 - val_loss: 0.1674\n",
            "Epoch 46/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0865 - val_loss: 0.1669\n",
            "Epoch 47/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0859 - val_loss: 0.1664\n",
            "Epoch 48/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0854 - val_loss: 0.1659\n",
            "Epoch 49/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0849 - val_loss: 0.1655\n",
            "Epoch 50/50\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0844 - val_loss: 0.1650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy of myeloic classifier model \n",
        "print(hc.graph.nodes['ILC_ALL']['memory'].training_acc)\n",
        "\n",
        "print(hc.graph.nodes['ILC_ALL']['memory'].test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_Mzndb1y2pt",
        "outputId": "6b1fd0b8-8679-4df7-d872-79f3f12dca53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9835886214442013\n",
            "0.9846827133479212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aktuell implementierte Objektauswahl macht Durchführung der Klassifizierung für 'Others' Knoten mit nur einem Tochterknoten sinnlos -> bräuchte direkte Ausgabe der softmax Aktivierungsfunktion um Wahrscheinlichkeiten auszugeben und dann eine confidence based entscheidung anstelle von auswahl des höchsten wertes (wäre dann immer eben der eine zur Wahl stehende Tochterknoten)"
      ],
      "metadata": {
        "id": "krNvN2Hszjpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Neural_Netwok.model.summary() shows summary of hidden and output layers and trainable parameter"
      ],
      "metadata": {
        "id": "qkFJXBlVXbSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6hQW1Xn-U9hn"
      }
    }
  ]
}